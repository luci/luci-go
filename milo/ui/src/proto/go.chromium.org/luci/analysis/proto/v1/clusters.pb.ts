// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.5
//   protoc               v6.30.0
// source: go.chromium.org/luci/analysis/proto/v1/clusters.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { Timestamp } from "../../../../../google/protobuf/timestamp.pb";
import {
  AssociatedBug,
  BuildStatus,
  buildStatusFromJSON,
  buildStatusToJSON,
  ClusterId,
  ExonerationReason,
  exonerationReasonFromJSON,
  exonerationReasonToJSON,
  PresubmitRunId,
  PresubmitRunMode,
  presubmitRunModeFromJSON,
  presubmitRunModeToJSON,
  PresubmitRunStatus,
  presubmitRunStatusFromJSON,
  presubmitRunStatusToJSON,
  TimeRange,
  Variant,
} from "./common.pb";
import { FailureReason } from "./failure_reason.pb";
import { Changelist, SourceRef } from "./sources.pb";

export const protobufPackage = "luci.analysis.v1";

export enum ClusterSummaryView {
  /**
   * CLUSTER_SUMMARY_VIEW_UNSPECIFIED - The default / unset value.
   * The API will default to the BASIC view.
   */
  CLUSTER_SUMMARY_VIEW_UNSPECIFIED = 0,
  /**
   * BASIC - Include most fields in the cluster summary, EXCLUDING
   * daily breakdowns of the cluster's impact metrics.
   */
  BASIC = 1,
  /** FULL - Include everything in the cluster summary. */
  FULL = 2,
}

export function clusterSummaryViewFromJSON(object: any): ClusterSummaryView {
  switch (object) {
    case 0:
    case "CLUSTER_SUMMARY_VIEW_UNSPECIFIED":
      return ClusterSummaryView.CLUSTER_SUMMARY_VIEW_UNSPECIFIED;
    case 1:
    case "BASIC":
      return ClusterSummaryView.BASIC;
    case 2:
    case "FULL":
      return ClusterSummaryView.FULL;
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum ClusterSummaryView");
  }
}

export function clusterSummaryViewToJSON(object: ClusterSummaryView): string {
  switch (object) {
    case ClusterSummaryView.CLUSTER_SUMMARY_VIEW_UNSPECIFIED:
      return "CLUSTER_SUMMARY_VIEW_UNSPECIFIED";
    case ClusterSummaryView.BASIC:
      return "BASIC";
    case ClusterSummaryView.FULL:
      return "FULL";
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum ClusterSummaryView");
  }
}

export interface ClusterRequest {
  /** The LUCI Project for which the test result should be clustered. */
  readonly project: string;
  /**
   * The test results to cluster. At most 1000 test results may be
   * clustered in one request.
   */
  readonly testResults: readonly ClusterRequest_TestResult[];
}

/**
 * TestResult captures information about a test result, sufficient to
 * cluster it. The fields requested here may be expanded over time.
 * For example, variant information may be requested in future.
 */
export interface ClusterRequest_TestResult {
  /**
   * Opaque tag supplied by the caller, to be returned in the
   * response. Provided to assist correlating responses with requests.
   * Does not need to be unique. Optional.
   */
  readonly requestTag: string;
  /**
   * Identifier of the test (as reported to ResultDB).
   * For chromium projects, this starts with ninja://.
   */
  readonly testId: string;
  /** The failure reason of the test (if any). */
  readonly failureReason: FailureReason | undefined;
}

export interface ClusterResponse {
  /**
   * The clusters each test result is in.
   * Contains one result for each test result specified in the request.
   * Results are provided in the same order as the request, so
   * the i-th ClusteredTestResult corresponds to the i-th
   * TestResult in the request.
   */
  readonly clusteredTestResults: readonly ClusterResponse_ClusteredTestResult[];
  /**
   * The versions of clustering algorithms, rules and project configuration
   * used to service this request. For debugging purposes only.
   */
  readonly clusteringVersion: ClusteringVersion | undefined;
}

/** The cluster(s) a test result is contained in. */
export interface ClusterResponse_ClusteredTestResult {
  /**
   * Opaque tag supplied by the caller in the request. Provided to assist
   * the caller correlate responses with requests.
   */
  readonly requestTag: string;
  /** The clusters the test result is contained within. */
  readonly clusters: readonly ClusterResponse_ClusteredTestResult_ClusterEntry[];
}

/** An individual cluster a test result is contained in. */
export interface ClusterResponse_ClusteredTestResult_ClusterEntry {
  /**
   * The unique identifier of the cluster.
   * If the algorithm is "rules", the cluster ID is also a rule ID.
   */
  readonly clusterId:
    | ClusterId
    | undefined;
  /**
   * The bug associated with the cluster, if any. This is only
   * populated for clusters defined by a failure association rule,
   * which associates specified failures to a bug.
   */
  readonly bug: AssociatedBug | undefined;
}

/**
 * The versions of algorithms, rules and configuration used by LUCI Analysis
 * to cluster test results. For a given test result and ClusteringVersion,
 * the set of returned clusters should always be the same.
 */
export interface ClusteringVersion {
  /** The version of clustering algorithms used. */
  readonly algorithmsVersion: number;
  /**
   * The version of failure association rules used. This is the Spanner
   * commit timestamp of the last rule modification incorporated in the
   * set of rules used to cluster the results.
   */
  readonly rulesVersion:
    | string
    | undefined;
  /**
   * The version of project configuration used. This is the timestamp
   * the project configuration was ingested by LUCI Analysis.
   */
  readonly configVersion: string | undefined;
}

export interface GetClusterRequest {
  /**
   * The resource name of the cluster to retrieve.
   * Format: projects/{project}/clusters/{cluster_algorithm}/{cluster_id}.
   * Designed to conform to aip.dev/131.
   */
  readonly name: string;
}

export interface Cluster {
  /**
   * The resource name of the cluster.
   * Format: projects/{project}/clusters/{cluster_algorithm}/{cluster_id}.
   */
  readonly name: string;
  /** Whether there is a recent example in the cluster. */
  readonly hasExample: boolean;
  /**
   * A human-readable name for the cluster.
   * Only populated for suggested clusters where has_example = true.
   * Not populated for rule-based clusters.
   */
  readonly title: string;
  /**
   * The values of metrics associated with the cluster. The map key is the ID
   * of the metric (e.g. "human-cls-failed-presubmit").
   *
   * The following metrics are currently defined:
   * - "human-cls-failed-presubmit":
   *   The number of distinct developer changelists that failed at least one
   *   presubmit (CQ) run because of failure(s) in this cluster. Excludes
   *   changelists authored by automation.
   * - "critical-failures-exonerated":
   *   The number of failures on test variants which were configured to be
   *   presubmit-blocking, which were exonerated (i.e. did not actually block
   *   presubmit) because infrastructure determined the test variant to be
   *   failing or too flaky at tip-of-tree. If this number is non-zero, it
   *   means a test variant which was configured to be presubmit-blocking is
   *   not stable enough to do so, and should be fixed or made non-blocking.
   * - "failures":
   *   The total number of test results in this cluster. LUCI Analysis only
   *   clusters test results which are unexpected and have a status of crash,
   *   abort or fail, so by definition the only test results counted here
   *   will be an unexpected fail/crash/abort.
   */
  readonly metrics: { [key: string]: Cluster_TimewiseCounts };
  /**
   * The failure association rule equivalent to the cluster. Populated only
   * for suggested clusters where has_example = true.
   * Not populated for rule-based clusters. If you need the failure
   * association rule for a rule-based cluster, use
   * luci.analysis.v1.Rules/Get to retrieve the rule with ID matching the
   * cluster ID.
   * Used to facilitate creating a new rule based on a suggested cluster.
   */
  readonly equivalentFailureAssociationRule: string;
}

export interface Cluster_Counts {
  /** The value of the metric (summed over all failures). */
  readonly nominal: string;
}

export interface Cluster_TimewiseCounts {
  /** The impact value for the last day. */
  readonly oneDay:
    | Cluster_Counts
    | undefined;
  /** The impact value for the last three days. */
  readonly threeDay:
    | Cluster_Counts
    | undefined;
  /** The impact value for the last week. */
  readonly sevenDay: Cluster_Counts | undefined;
}

export interface Cluster_MetricsEntry {
  readonly key: string;
  readonly value: Cluster_TimewiseCounts | undefined;
}

/** Designed to conform with aip.dev/131. */
export interface GetReclusteringProgressRequest {
  /**
   * The name of the reclustering progress resource to retrieve.
   * Format: projects/{project}/reclusteringProgress.
   */
  readonly name: string;
}

/**
 * ReclusteringProgress captures the progress re-clustering a
 * given LUCI project's test results using specific rules
 * versions or algorithms versions.
 */
export interface ReclusteringProgress {
  /**
   * The name of the reclustering progress resource.
   * Format: projects/{project}/reclusteringProgress.
   */
  readonly name: string;
  /**
   * ProgressPerMille is the progress of the current re-clustering run,
   * measured in thousandths (per mille). As such, this value ranges
   * from 0 (0% complete) to 1000 (100% complete).
   */
  readonly progressPerMille: number;
  /** The goal of the last completed re-clustering run. */
  readonly last:
    | ClusteringVersion
    | undefined;
  /**
   * The goal of the current re-clustering run. (For which
   * ProgressPerMille is specified.) This may be the same as the
   * last completed re-clustering run the available algorithm versions,
   * rules and configuration is unchanged.
   */
  readonly next: ClusteringVersion | undefined;
}

export interface QueryClusterSummariesRequest {
  /** The LUCI Project. */
  readonly project: string;
  /**
   * An AIP-160 style filter to select test failures in the project
   * to cluster and calculate metrics for.
   *
   * Filtering supports a subset of [AIP-160 filtering](https://google.aip.dev/160).
   *
   * All values are case-sensitive.
   *
   * A bare value is searched for in the columns test_id and
   * failure_reason. E.g. ninja or "test failed".
   *
   * You can use AND, OR and NOT (case sensitive) logical operators, along
   * with grouping. '-' is equivalent to NOT. Multiple bare values are
   * considered to be AND separated.  E.g. These are equivalent:
   * hello world
   * and:
   * hello AND world
   *
   * More examples:
   * a OR b
   * a AND NOT(b or -c)
   *
   * You can filter particular columns with '=', '!=' and ':' (has) operators.
   * The right hand side of the operator must be a simple value. E.g:
   * test_id:telemetry
   * -failure_reason:Timeout
   * ingested_invocation_id="build-8822963500388678513"
   *
   * Supported columns to search on:
   * - test_id
   * - failure_reason
   * - realm
   * - ingested_invocation_id
   * - cluster_algorithm
   * - cluster_id
   * - variant_hash
   * - test_run_id
   * - tags
   *
   * Note that cost is greatly reduced (more than 90%) if exact matches for the
   * cluster_algorithm and cluster_id field are both provided in the filter string.
   */
  readonly failureFilter: string;
  /**
   * A comma-separated list of fields to order the response by.
   *
   * The default sorting order is ascending; to specify descending order
   * for a field append a " desc" suffix. The dot syntax can be used
   * to navigate fields and map keys, and the backtick character (``) used
   * to escape field names that do not match `[a-zA-Z_][a-zA-Z0-9_]`.
   *
   * The only sortable columns that are supported currently are metric
   * fields.
   *
   * For example, to sort by human CLs failed presubmit descending, use:
   * "metrics.`human-cls-failed-presubmit`.value desc".
   * To sort by human CLs failed presubmit followed by failures, use:
   * "metrics.`human-cls-failed-presubmit`.value desc, metrics.`failures`.value desc"
   *
   * For more details, see aip.dev/132 for ordering syntax, and
   * aip.dev/161#map-fields for navigating map fields.
   */
  readonly orderBy: string;
  /**
   * The resource name(s) of the metrics to include in the cluster summaries.
   * Format: projects/{project}/metrics/{metric_id}.
   * See the metrics field on the luci.analysis.v1.Cluster message for details
   * about valid metric identifiers.
   */
  readonly metrics: readonly string[];
  /**
   * The time range over which to get the cluster summaries.
   * Note: the response will include only data for the portion of the
   * time range that is within the data retention period of 90 days.
   */
  readonly timeRange:
    | TimeRange
    | undefined;
  /**
   * The level of detail that the returned cluster summaries should have. See
   * luci.analysis.v1.ClusterSummaryView.
   */
  readonly view: ClusterSummaryView;
}

export interface QueryClusterSummariesResponse {
  /** The clusters and impact metrics from the filtered failures. */
  readonly clusterSummaries: readonly ClusterSummary[];
}

export interface ClusterSummary {
  /** The cluster ID of this cluster. */
  readonly clusterId:
    | ClusterId
    | undefined;
  /** Title is a one-line description of the cluster. */
  readonly title: string;
  /**
   * The bug associated with the cluster. This will only be present for
   * rules algorithm clusters.
   */
  readonly bug:
    | AssociatedBug
    | undefined;
  /**
   * The values of cluster metrics. The key of the map is the identifier
   * of the metric (e.g. "human-cls-failed-presubmit").
   * See the metrics field on the luci.analysis.v1.Cluster message for details
   * about valid metric identifiers.
   */
  readonly metrics: { [key: string]: ClusterSummary_MetricValue };
}

export interface ClusterSummary_MetricValue {
  /**
   * The residual value of the cluster metric.
   * For bug clusters, the residual metric value is the metric value
   * calculated using all of the failures in the cluster.
   * For suggested clusters, the residual metric value is calculated
   * using the failures in the cluster which are not also part of a
   * bug cluster. In this way, measures attributed to bug clusters
   * are not counted again against suggested clusters.
   */
  readonly value: string;
  /**
   * The value of the cluster metric over time, grouped by 24-hour periods
   * in the queried time range, in reverse chronological order
   * i.e. the first entry is the metric value for the 24-hour period
   * immediately preceding the time range's latest time.
   */
  readonly dailyBreakdown: readonly string[];
}

export interface ClusterSummary_MetricsEntry {
  readonly key: string;
  readonly value: ClusterSummary_MetricValue | undefined;
}

export interface QueryClusterFailuresRequest {
  /**
   * The resource name of the cluster failures to retrieve.
   * Format: projects/{project}/clusters/{cluster_algorithm}/{cluster_id}/failures.
   */
  readonly parent: string;
  /**
   * Optional. The resource name of the metric for which failures should
   * be displayed.
   * Format: projects/{project}/metrics/{metric_id}.
   *
   * If no metrics is specified here, then no filtering is performed
   * and all failures are eligible to be returned. Over time, we may wish
   * to migrate this to an AIP-160 filter clause, e.g. "in_metric(`metric-id`)"
   * where in_metric is a function.
   */
  readonly metricFilter: string;
}

export interface QueryClusterFailuresResponse {
  /**
   * Example failures in the cluster.
   * Limited to the most recent 2000 examples.
   */
  readonly failures: readonly DistinctClusterFailure[];
}

/**
 * DistinctClusterFailure represents a number of failures which have identical
 * properties. This provides slightly compressed transfer of examples.
 */
export interface DistinctClusterFailure {
  /** The identity of the test. */
  readonly testId: string;
  /**
   * Description of one specific way of running the test,
   * e.g. a specific bucket, builder and a test suite.
   */
  readonly variant:
    | Variant
    | undefined;
  /**
   * Timestamp representing the start of the data retention period for the
   * test results in this group.
   * The partition time is the creation time of the ingested invocation in
   * Spanner.
   */
  readonly partitionTime:
    | string
    | undefined;
  /** Details if the presubmit run associated with these results (if any). */
  readonly presubmitRun:
    | DistinctClusterFailure_PresubmitRun
    | undefined;
  /**
   * Whether the build was critical to a presubmit run succeeding.
   * If the build was not part of a presubmit run, this field should
   * be ignored.
   */
  readonly isBuildCritical: boolean;
  /** The exonerations applied to the test variant verdict. */
  readonly exonerations: readonly DistinctClusterFailure_Exoneration[];
  /**
   * The status of the build that contained this test result. Can be used
   * to filter incomplete results (e.g. where build was cancelled or had
   * an infra failure). Can also be used to filter builds with incomplete
   * exonerations (e.g. build succeeded but some tests not exonerated).
   * This is the build corresponding to ingested_invocation_id.
   */
  readonly buildStatus: BuildStatus;
  /**
   * The invocation from which this test result was ingested. This is
   * the top-level invocation that was ingested, an "invocation" being
   * a container of test results as identified by the source test result
   * system.
   *
   * For ResultDB, LUCI Analysis ingests invocations corresponding to
   * buildbucket builds.
   */
  readonly ingestedInvocationId: string;
  /**
   * Is the ingested invocation blocked by this test variant? This is
   * only true if all (non-skipped) test results for this test variant
   * (in the ingested invocation) are unexpected failures.
   *
   * Exoneration does not factor into this value; check exonerations
   * to see if the impact of this ingested invocation being blocked was
   * mitigated by exoneration.
   */
  readonly isIngestedInvocationBlocked: boolean;
  /**
   * The unsubmitted changelists that were tested (if any).
   * Up to 10 changelists are captured.
   */
  readonly changelists: readonly Changelist[];
  /** The number of test results which have these properties. */
  readonly count: number;
  /**
   * The first 255 characters of the failure_reason.primary_error_message
   * field of one of the test results.
   * Note that this is for saving the user a click in the UI, not for
   * analytical purposes.
   */
  readonly failureReasonPrefix: string;
}

/**
 * Representation of an exoneration. An exoneration means the subject of
 * the test (e.g. a CL) is absolved from blame for the unexpected results
 * of the test variant.
 */
export interface DistinctClusterFailure_Exoneration {
  /** The machine-readable reason for the exoneration. */
  readonly reason: ExonerationReason;
}

/** Representation of a presubmit run (e.g. LUCI CV Run). */
export interface DistinctClusterFailure_PresubmitRun {
  /**
   * Identity of the presubmit run that contains this test result.
   * This should be unique per "CQ+1"/"CQ+2" attempt on gerrit.
   *
   * One presubmit run MAY have many ingested invocation IDs (e.g. for its
   * various tryjobs), but every ingested invocation ID only ever has one
   * presubmit run ID (if any).
   *
   * If the test result was not collected as part of a presubmit run,
   * this is unset.
   */
  readonly presubmitRunId:
    | PresubmitRunId
    | undefined;
  /**
   * The owner of the presubmit run (if any).
   * This is the owner of the CL on which CQ+1/CQ+2 was clicked
   * (even in case of presubmit run with multiple CLs).
   * There is scope for this field to become an email address if privacy
   * approval is obtained, until then it is "automation" (for automation
   * service accounts) and "user" otherwise.
   */
  readonly owner: string;
  /** The mode of the presubmit run. E.g. DRY_RUN, FULL_RUN, QUICK_DRY_RUN. */
  readonly mode: PresubmitRunMode;
  /** The status of the presubmit run. E.g. succeeded, failed or cancelled. */
  readonly status: PresubmitRunStatus;
}

export interface QueryClusterExoneratedTestVariantsRequest {
  /**
   * The resource name of the cluster exonerated test variants to retrieve.
   * Format: projects/{project}/clusters/{cluster_algorithm}/{cluster_id}/exoneratedTestVariants.
   */
  readonly parent: string;
}

export interface QueryClusterExoneratedTestVariantsResponse {
  /**
   * A list of test variants in the cluster which have exonerated critical
   * failures. Ordered by recency of the exoneration (most recent exonerations
   * first) and limited to at most 100 test variants.
   */
  readonly testVariants: readonly ClusterExoneratedTestVariant[];
}

/**
 * ClusterExoneratedTestVariant represents a test variant in a cluster
 * which has been exonerated. A cluster test variant is the subset
 * of a test variant that intersects with the failures of a cluster.
 */
export interface ClusterExoneratedTestVariant {
  /** A unique identifier of the test in a LUCI project. */
  readonly testId: string;
  /**
   * Description of one specific way of running the test,
   * e.g. a specific bucket, builder and a test suite.
   */
  readonly variant:
    | Variant
    | undefined;
  /**
   * The number of critical (presubmit-blocking) failures in the
   * cluster which have been exonerated on this test variant
   * in the last week.
   */
  readonly criticalFailuresExonerated: number;
  /**
   * The partition time of the most recent exoneration of a
   * critical failure.
   */
  readonly lastExoneration: string | undefined;
}

export interface QueryClusterExoneratedTestVariantBranchesRequest {
  /**
   * The resource name of the cluster exonerated test variant branches to retrieve.
   * Format: projects/{project}/clusters/{cluster_algorithm}/{cluster_id}/exoneratedTestVariantBranches.
   */
  readonly parent: string;
}

export interface QueryClusterExoneratedTestVariantBranchesResponse {
  /**
   * A list of test variants branches in the cluster which have exonerated
   * critical failures. Ordered by recency of the exoneration (most recent
   * exonerations first) and limited to at most 100 test variant branches.
   *
   * Pagination following AIP-158 may be implemented in future if
   * more than 100 items is needed.
   */
  readonly testVariantBranches: readonly ClusterExoneratedTestVariantBranch[];
}

/**
 * ClusterExoneratedTestVariantBranch represents a (test, variant, source ref)
 * in a cluster which has been exonerated. A cluster test variant branch is
 * the subset of a test variant branch that intersects with the failures of a
 * cluster.
 */
export interface ClusterExoneratedTestVariantBranch {
  /** The LUCI project. */
  readonly project: string;
  /** A unique identifier of the test in a LUCI project. */
  readonly testId: string;
  /**
   * Description of one specific way of running the test,
   * e.g. a specific bucket, builder and a test suite.
   */
  readonly variant:
    | Variant
    | undefined;
  /**
   * The branch in source control that was tested, if known.
   * For example, the `refs/heads/main` branch in the `chromium/src` repo
   * hosted by `chromium.googlesource.com`.
   */
  readonly sourceRef:
    | SourceRef
    | undefined;
  /**
   * The number of critical (presubmit-blocking) failures in the
   * cluster which have been exonerated on this test variant
   * in the last week.
   */
  readonly criticalFailuresExonerated: number;
  /**
   * The partition time of the most recent exoneration of a
   * critical failure.
   */
  readonly lastExoneration: string | undefined;
}

export interface QueryClusterHistoryRequest {
  /** The LUCI Project. */
  readonly project: string;
  /**
   * An AIP-160 style filter to select test failures in the project
   * to calculate metrics for.
   *
   * See the description of the QueryClusterSummariesRequest.failure_filter
   * above for the format of this field.
   *
   * Note that cost is greatly reduced (more than 90%) if exact matches for the
   * cluster_algorithm and cluster_id field are both provided in the filter string.
   */
  readonly failureFilter: string;
  /**
   * The number of days of history to return.  Maximum of 90 as only 90 days of
   * history is kept by LUCI Analysis.  Note that the cost of the query scales
   * linearly with the number of days.
   */
  readonly days: number;
  /**
   * The resource name(s) of the metrics to include in the cluster histories.
   * Format: projects/{project}/metrics/{metric_id}.
   * See the metrics field on the luci.analysis.v1.Cluster message for details
   * about valid metric identifiers.
   */
  readonly metrics: readonly string[];
}

export interface QueryClusterHistoryResponse {
  /**
   * The metrics for each day.  There will be the same number of days as
   * requested in the request.  The entries will be returned in sorted date
   * order, earliest day first.
   */
  readonly days: readonly ClusterHistoryDay[];
}

/** Represents metrics about a cluster on a specific day. */
export interface ClusterHistoryDay {
  /**
   * A map from requested metric name to the value of that metric on this day.
   * The key of the map is the metric ID.
   */
  readonly metrics: { [key: string]: number };
  /**
   * The date that these metrics are for.
   * This is a UTC date in ISO 8601 format, e.g. 2022-11-29
   */
  readonly date: string;
}

export interface ClusterHistoryDay_MetricsEntry {
  readonly key: string;
  readonly value: number;
}

function createBaseClusterRequest(): ClusterRequest {
  return { project: "", testResults: [] };
}

export const ClusterRequest: MessageFns<ClusterRequest> = {
  encode(message: ClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.project !== "") {
      writer.uint32(10).string(message.project);
    }
    for (const v of message.testResults) {
      ClusterRequest_TestResult.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.project = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.testResults.push(ClusterRequest_TestResult.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterRequest {
    return {
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      testResults: globalThis.Array.isArray(object?.testResults)
        ? object.testResults.map((e: any) => ClusterRequest_TestResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ClusterRequest): unknown {
    const obj: any = {};
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.testResults?.length) {
      obj.testResults = message.testResults.map((e) => ClusterRequest_TestResult.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterRequest>): ClusterRequest {
    return ClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterRequest>): ClusterRequest {
    const message = createBaseClusterRequest() as any;
    message.project = object.project ?? "";
    message.testResults = object.testResults?.map((e) => ClusterRequest_TestResult.fromPartial(e)) || [];
    return message;
  },
};

function createBaseClusterRequest_TestResult(): ClusterRequest_TestResult {
  return { requestTag: "", testId: "", failureReason: undefined };
}

export const ClusterRequest_TestResult: MessageFns<ClusterRequest_TestResult> = {
  encode(message: ClusterRequest_TestResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.requestTag !== "") {
      writer.uint32(10).string(message.requestTag);
    }
    if (message.testId !== "") {
      writer.uint32(18).string(message.testId);
    }
    if (message.failureReason !== undefined) {
      FailureReason.encode(message.failureReason, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterRequest_TestResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterRequest_TestResult() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.requestTag = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.testId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.failureReason = FailureReason.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterRequest_TestResult {
    return {
      requestTag: isSet(object.requestTag) ? globalThis.String(object.requestTag) : "",
      testId: isSet(object.testId) ? globalThis.String(object.testId) : "",
      failureReason: isSet(object.failureReason) ? FailureReason.fromJSON(object.failureReason) : undefined,
    };
  },

  toJSON(message: ClusterRequest_TestResult): unknown {
    const obj: any = {};
    if (message.requestTag !== "") {
      obj.requestTag = message.requestTag;
    }
    if (message.testId !== "") {
      obj.testId = message.testId;
    }
    if (message.failureReason !== undefined) {
      obj.failureReason = FailureReason.toJSON(message.failureReason);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterRequest_TestResult>): ClusterRequest_TestResult {
    return ClusterRequest_TestResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterRequest_TestResult>): ClusterRequest_TestResult {
    const message = createBaseClusterRequest_TestResult() as any;
    message.requestTag = object.requestTag ?? "";
    message.testId = object.testId ?? "";
    message.failureReason = (object.failureReason !== undefined && object.failureReason !== null)
      ? FailureReason.fromPartial(object.failureReason)
      : undefined;
    return message;
  },
};

function createBaseClusterResponse(): ClusterResponse {
  return { clusteredTestResults: [], clusteringVersion: undefined };
}

export const ClusterResponse: MessageFns<ClusterResponse> = {
  encode(message: ClusterResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.clusteredTestResults) {
      ClusterResponse_ClusteredTestResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.clusteringVersion !== undefined) {
      ClusteringVersion.encode(message.clusteringVersion, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.clusteredTestResults.push(ClusterResponse_ClusteredTestResult.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.clusteringVersion = ClusteringVersion.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterResponse {
    return {
      clusteredTestResults: globalThis.Array.isArray(object?.clusteredTestResults)
        ? object.clusteredTestResults.map((e: any) => ClusterResponse_ClusteredTestResult.fromJSON(e))
        : [],
      clusteringVersion: isSet(object.clusteringVersion)
        ? ClusteringVersion.fromJSON(object.clusteringVersion)
        : undefined,
    };
  },

  toJSON(message: ClusterResponse): unknown {
    const obj: any = {};
    if (message.clusteredTestResults?.length) {
      obj.clusteredTestResults = message.clusteredTestResults.map((e) => ClusterResponse_ClusteredTestResult.toJSON(e));
    }
    if (message.clusteringVersion !== undefined) {
      obj.clusteringVersion = ClusteringVersion.toJSON(message.clusteringVersion);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterResponse>): ClusterResponse {
    return ClusterResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterResponse>): ClusterResponse {
    const message = createBaseClusterResponse() as any;
    message.clusteredTestResults =
      object.clusteredTestResults?.map((e) => ClusterResponse_ClusteredTestResult.fromPartial(e)) || [];
    message.clusteringVersion = (object.clusteringVersion !== undefined && object.clusteringVersion !== null)
      ? ClusteringVersion.fromPartial(object.clusteringVersion)
      : undefined;
    return message;
  },
};

function createBaseClusterResponse_ClusteredTestResult(): ClusterResponse_ClusteredTestResult {
  return { requestTag: "", clusters: [] };
}

export const ClusterResponse_ClusteredTestResult: MessageFns<ClusterResponse_ClusteredTestResult> = {
  encode(message: ClusterResponse_ClusteredTestResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.requestTag !== "") {
      writer.uint32(10).string(message.requestTag);
    }
    for (const v of message.clusters) {
      ClusterResponse_ClusteredTestResult_ClusterEntry.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterResponse_ClusteredTestResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterResponse_ClusteredTestResult() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.requestTag = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.clusters.push(ClusterResponse_ClusteredTestResult_ClusterEntry.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterResponse_ClusteredTestResult {
    return {
      requestTag: isSet(object.requestTag) ? globalThis.String(object.requestTag) : "",
      clusters: globalThis.Array.isArray(object?.clusters)
        ? object.clusters.map((e: any) => ClusterResponse_ClusteredTestResult_ClusterEntry.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ClusterResponse_ClusteredTestResult): unknown {
    const obj: any = {};
    if (message.requestTag !== "") {
      obj.requestTag = message.requestTag;
    }
    if (message.clusters?.length) {
      obj.clusters = message.clusters.map((e) => ClusterResponse_ClusteredTestResult_ClusterEntry.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterResponse_ClusteredTestResult>): ClusterResponse_ClusteredTestResult {
    return ClusterResponse_ClusteredTestResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterResponse_ClusteredTestResult>): ClusterResponse_ClusteredTestResult {
    const message = createBaseClusterResponse_ClusteredTestResult() as any;
    message.requestTag = object.requestTag ?? "";
    message.clusters = object.clusters?.map((e) => ClusterResponse_ClusteredTestResult_ClusterEntry.fromPartial(e)) ||
      [];
    return message;
  },
};

function createBaseClusterResponse_ClusteredTestResult_ClusterEntry(): ClusterResponse_ClusteredTestResult_ClusterEntry {
  return { clusterId: undefined, bug: undefined };
}

export const ClusterResponse_ClusteredTestResult_ClusterEntry: MessageFns<
  ClusterResponse_ClusteredTestResult_ClusterEntry
> = {
  encode(
    message: ClusterResponse_ClusteredTestResult_ClusterEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.clusterId !== undefined) {
      ClusterId.encode(message.clusterId, writer.uint32(10).fork()).join();
    }
    if (message.bug !== undefined) {
      AssociatedBug.encode(message.bug, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterResponse_ClusteredTestResult_ClusterEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterResponse_ClusteredTestResult_ClusterEntry() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.clusterId = ClusterId.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.bug = AssociatedBug.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterResponse_ClusteredTestResult_ClusterEntry {
    return {
      clusterId: isSet(object.clusterId) ? ClusterId.fromJSON(object.clusterId) : undefined,
      bug: isSet(object.bug) ? AssociatedBug.fromJSON(object.bug) : undefined,
    };
  },

  toJSON(message: ClusterResponse_ClusteredTestResult_ClusterEntry): unknown {
    const obj: any = {};
    if (message.clusterId !== undefined) {
      obj.clusterId = ClusterId.toJSON(message.clusterId);
    }
    if (message.bug !== undefined) {
      obj.bug = AssociatedBug.toJSON(message.bug);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ClusterResponse_ClusteredTestResult_ClusterEntry>,
  ): ClusterResponse_ClusteredTestResult_ClusterEntry {
    return ClusterResponse_ClusteredTestResult_ClusterEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ClusterResponse_ClusteredTestResult_ClusterEntry>,
  ): ClusterResponse_ClusteredTestResult_ClusterEntry {
    const message = createBaseClusterResponse_ClusteredTestResult_ClusterEntry() as any;
    message.clusterId = (object.clusterId !== undefined && object.clusterId !== null)
      ? ClusterId.fromPartial(object.clusterId)
      : undefined;
    message.bug = (object.bug !== undefined && object.bug !== null) ? AssociatedBug.fromPartial(object.bug) : undefined;
    return message;
  },
};

function createBaseClusteringVersion(): ClusteringVersion {
  return { algorithmsVersion: 0, rulesVersion: undefined, configVersion: undefined };
}

export const ClusteringVersion: MessageFns<ClusteringVersion> = {
  encode(message: ClusteringVersion, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.algorithmsVersion !== 0) {
      writer.uint32(8).int32(message.algorithmsVersion);
    }
    if (message.rulesVersion !== undefined) {
      Timestamp.encode(toTimestamp(message.rulesVersion), writer.uint32(18).fork()).join();
    }
    if (message.configVersion !== undefined) {
      Timestamp.encode(toTimestamp(message.configVersion), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusteringVersion {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusteringVersion() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.algorithmsVersion = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.rulesVersion = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.configVersion = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusteringVersion {
    return {
      algorithmsVersion: isSet(object.algorithmsVersion) ? globalThis.Number(object.algorithmsVersion) : 0,
      rulesVersion: isSet(object.rulesVersion) ? globalThis.String(object.rulesVersion) : undefined,
      configVersion: isSet(object.configVersion) ? globalThis.String(object.configVersion) : undefined,
    };
  },

  toJSON(message: ClusteringVersion): unknown {
    const obj: any = {};
    if (message.algorithmsVersion !== 0) {
      obj.algorithmsVersion = Math.round(message.algorithmsVersion);
    }
    if (message.rulesVersion !== undefined) {
      obj.rulesVersion = message.rulesVersion;
    }
    if (message.configVersion !== undefined) {
      obj.configVersion = message.configVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<ClusteringVersion>): ClusteringVersion {
    return ClusteringVersion.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusteringVersion>): ClusteringVersion {
    const message = createBaseClusteringVersion() as any;
    message.algorithmsVersion = object.algorithmsVersion ?? 0;
    message.rulesVersion = object.rulesVersion ?? undefined;
    message.configVersion = object.configVersion ?? undefined;
    return message;
  },
};

function createBaseGetClusterRequest(): GetClusterRequest {
  return { name: "" };
}

export const GetClusterRequest: MessageFns<GetClusterRequest> = {
  encode(message: GetClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetClusterRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetClusterRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetClusterRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetClusterRequest>): GetClusterRequest {
    return GetClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetClusterRequest>): GetClusterRequest {
    const message = createBaseGetClusterRequest() as any;
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseCluster(): Cluster {
  return { name: "", hasExample: false, title: "", metrics: {}, equivalentFailureAssociationRule: "" };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.hasExample !== false) {
      writer.uint32(16).bool(message.hasExample);
    }
    if (message.title !== "") {
      writer.uint32(26).string(message.title);
    }
    Object.entries(message.metrics).forEach(([key, value]) => {
      Cluster_MetricsEntry.encode({ key: key as any, value }, writer.uint32(82).fork()).join();
    });
    if (message.equivalentFailureAssociationRule !== "") {
      writer.uint32(58).string(message.equivalentFailureAssociationRule);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.hasExample = reader.bool();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.title = reader.string();
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          const entry10 = Cluster_MetricsEntry.decode(reader, reader.uint32());
          if (entry10.value !== undefined) {
            message.metrics[entry10.key] = entry10.value;
          }
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.equivalentFailureAssociationRule = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      hasExample: isSet(object.hasExample) ? globalThis.Boolean(object.hasExample) : false,
      title: isSet(object.title) ? globalThis.String(object.title) : "",
      metrics: isObject(object.metrics)
        ? Object.entries(object.metrics).reduce<{ [key: string]: Cluster_TimewiseCounts }>((acc, [key, value]) => {
          acc[key] = Cluster_TimewiseCounts.fromJSON(value);
          return acc;
        }, {})
        : {},
      equivalentFailureAssociationRule: isSet(object.equivalentFailureAssociationRule)
        ? globalThis.String(object.equivalentFailureAssociationRule)
        : "",
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.hasExample !== false) {
      obj.hasExample = message.hasExample;
    }
    if (message.title !== "") {
      obj.title = message.title;
    }
    if (message.metrics) {
      const entries = Object.entries(message.metrics);
      if (entries.length > 0) {
        obj.metrics = {};
        entries.forEach(([k, v]) => {
          obj.metrics[k] = Cluster_TimewiseCounts.toJSON(v);
        });
      }
    }
    if (message.equivalentFailureAssociationRule !== "") {
      obj.equivalentFailureAssociationRule = message.equivalentFailureAssociationRule;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster() as any;
    message.name = object.name ?? "";
    message.hasExample = object.hasExample ?? false;
    message.title = object.title ?? "";
    message.metrics = Object.entries(object.metrics ?? {}).reduce<{ [key: string]: Cluster_TimewiseCounts }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Cluster_TimewiseCounts.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.equivalentFailureAssociationRule = object.equivalentFailureAssociationRule ?? "";
    return message;
  },
};

function createBaseCluster_Counts(): Cluster_Counts {
  return { nominal: "0" };
}

export const Cluster_Counts: MessageFns<Cluster_Counts> = {
  encode(message: Cluster_Counts, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nominal !== "0") {
      writer.uint32(8).int64(message.nominal);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_Counts {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_Counts() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.nominal = reader.int64().toString();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_Counts {
    return { nominal: isSet(object.nominal) ? globalThis.String(object.nominal) : "0" };
  },

  toJSON(message: Cluster_Counts): unknown {
    const obj: any = {};
    if (message.nominal !== "0") {
      obj.nominal = message.nominal;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_Counts>): Cluster_Counts {
    return Cluster_Counts.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_Counts>): Cluster_Counts {
    const message = createBaseCluster_Counts() as any;
    message.nominal = object.nominal ?? "0";
    return message;
  },
};

function createBaseCluster_TimewiseCounts(): Cluster_TimewiseCounts {
  return { oneDay: undefined, threeDay: undefined, sevenDay: undefined };
}

export const Cluster_TimewiseCounts: MessageFns<Cluster_TimewiseCounts> = {
  encode(message: Cluster_TimewiseCounts, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.oneDay !== undefined) {
      Cluster_Counts.encode(message.oneDay, writer.uint32(18).fork()).join();
    }
    if (message.threeDay !== undefined) {
      Cluster_Counts.encode(message.threeDay, writer.uint32(26).fork()).join();
    }
    if (message.sevenDay !== undefined) {
      Cluster_Counts.encode(message.sevenDay, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_TimewiseCounts {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_TimewiseCounts() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.oneDay = Cluster_Counts.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.threeDay = Cluster_Counts.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.sevenDay = Cluster_Counts.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_TimewiseCounts {
    return {
      oneDay: isSet(object.oneDay) ? Cluster_Counts.fromJSON(object.oneDay) : undefined,
      threeDay: isSet(object.threeDay) ? Cluster_Counts.fromJSON(object.threeDay) : undefined,
      sevenDay: isSet(object.sevenDay) ? Cluster_Counts.fromJSON(object.sevenDay) : undefined,
    };
  },

  toJSON(message: Cluster_TimewiseCounts): unknown {
    const obj: any = {};
    if (message.oneDay !== undefined) {
      obj.oneDay = Cluster_Counts.toJSON(message.oneDay);
    }
    if (message.threeDay !== undefined) {
      obj.threeDay = Cluster_Counts.toJSON(message.threeDay);
    }
    if (message.sevenDay !== undefined) {
      obj.sevenDay = Cluster_Counts.toJSON(message.sevenDay);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_TimewiseCounts>): Cluster_TimewiseCounts {
    return Cluster_TimewiseCounts.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_TimewiseCounts>): Cluster_TimewiseCounts {
    const message = createBaseCluster_TimewiseCounts() as any;
    message.oneDay = (object.oneDay !== undefined && object.oneDay !== null)
      ? Cluster_Counts.fromPartial(object.oneDay)
      : undefined;
    message.threeDay = (object.threeDay !== undefined && object.threeDay !== null)
      ? Cluster_Counts.fromPartial(object.threeDay)
      : undefined;
    message.sevenDay = (object.sevenDay !== undefined && object.sevenDay !== null)
      ? Cluster_Counts.fromPartial(object.sevenDay)
      : undefined;
    return message;
  },
};

function createBaseCluster_MetricsEntry(): Cluster_MetricsEntry {
  return { key: "", value: undefined };
}

export const Cluster_MetricsEntry: MessageFns<Cluster_MetricsEntry> = {
  encode(message: Cluster_MetricsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Cluster_TimewiseCounts.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_MetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_MetricsEntry() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = Cluster_TimewiseCounts.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_MetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Cluster_TimewiseCounts.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: Cluster_MetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Cluster_TimewiseCounts.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_MetricsEntry>): Cluster_MetricsEntry {
    return Cluster_MetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_MetricsEntry>): Cluster_MetricsEntry {
    const message = createBaseCluster_MetricsEntry() as any;
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? Cluster_TimewiseCounts.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseGetReclusteringProgressRequest(): GetReclusteringProgressRequest {
  return { name: "" };
}

export const GetReclusteringProgressRequest: MessageFns<GetReclusteringProgressRequest> = {
  encode(message: GetReclusteringProgressRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetReclusteringProgressRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetReclusteringProgressRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetReclusteringProgressRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetReclusteringProgressRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetReclusteringProgressRequest>): GetReclusteringProgressRequest {
    return GetReclusteringProgressRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetReclusteringProgressRequest>): GetReclusteringProgressRequest {
    const message = createBaseGetReclusteringProgressRequest() as any;
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseReclusteringProgress(): ReclusteringProgress {
  return { name: "", progressPerMille: 0, last: undefined, next: undefined };
}

export const ReclusteringProgress: MessageFns<ReclusteringProgress> = {
  encode(message: ReclusteringProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.progressPerMille !== 0) {
      writer.uint32(16).int32(message.progressPerMille);
    }
    if (message.last !== undefined) {
      ClusteringVersion.encode(message.last, writer.uint32(42).fork()).join();
    }
    if (message.next !== undefined) {
      ClusteringVersion.encode(message.next, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReclusteringProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReclusteringProgress() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.progressPerMille = reader.int32();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.last = ClusteringVersion.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.next = ClusteringVersion.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReclusteringProgress {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      progressPerMille: isSet(object.progressPerMille) ? globalThis.Number(object.progressPerMille) : 0,
      last: isSet(object.last) ? ClusteringVersion.fromJSON(object.last) : undefined,
      next: isSet(object.next) ? ClusteringVersion.fromJSON(object.next) : undefined,
    };
  },

  toJSON(message: ReclusteringProgress): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.progressPerMille !== 0) {
      obj.progressPerMille = Math.round(message.progressPerMille);
    }
    if (message.last !== undefined) {
      obj.last = ClusteringVersion.toJSON(message.last);
    }
    if (message.next !== undefined) {
      obj.next = ClusteringVersion.toJSON(message.next);
    }
    return obj;
  },

  create(base?: DeepPartial<ReclusteringProgress>): ReclusteringProgress {
    return ReclusteringProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReclusteringProgress>): ReclusteringProgress {
    const message = createBaseReclusteringProgress() as any;
    message.name = object.name ?? "";
    message.progressPerMille = object.progressPerMille ?? 0;
    message.last = (object.last !== undefined && object.last !== null)
      ? ClusteringVersion.fromPartial(object.last)
      : undefined;
    message.next = (object.next !== undefined && object.next !== null)
      ? ClusteringVersion.fromPartial(object.next)
      : undefined;
    return message;
  },
};

function createBaseQueryClusterSummariesRequest(): QueryClusterSummariesRequest {
  return { project: "", failureFilter: "", orderBy: "", metrics: [], timeRange: undefined, view: 0 };
}

export const QueryClusterSummariesRequest: MessageFns<QueryClusterSummariesRequest> = {
  encode(message: QueryClusterSummariesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.project !== "") {
      writer.uint32(10).string(message.project);
    }
    if (message.failureFilter !== "") {
      writer.uint32(18).string(message.failureFilter);
    }
    if (message.orderBy !== "") {
      writer.uint32(26).string(message.orderBy);
    }
    for (const v of message.metrics) {
      writer.uint32(34).string(v!);
    }
    if (message.timeRange !== undefined) {
      TimeRange.encode(message.timeRange, writer.uint32(42).fork()).join();
    }
    if (message.view !== 0) {
      writer.uint32(48).int32(message.view);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterSummariesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterSummariesRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.project = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.failureFilter = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.orderBy = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.metrics.push(reader.string());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.timeRange = TimeRange.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.view = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterSummariesRequest {
    return {
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      failureFilter: isSet(object.failureFilter) ? globalThis.String(object.failureFilter) : "",
      orderBy: isSet(object.orderBy) ? globalThis.String(object.orderBy) : "",
      metrics: globalThis.Array.isArray(object?.metrics) ? object.metrics.map((e: any) => globalThis.String(e)) : [],
      timeRange: isSet(object.timeRange) ? TimeRange.fromJSON(object.timeRange) : undefined,
      view: isSet(object.view) ? clusterSummaryViewFromJSON(object.view) : 0,
    };
  },

  toJSON(message: QueryClusterSummariesRequest): unknown {
    const obj: any = {};
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.failureFilter !== "") {
      obj.failureFilter = message.failureFilter;
    }
    if (message.orderBy !== "") {
      obj.orderBy = message.orderBy;
    }
    if (message.metrics?.length) {
      obj.metrics = message.metrics;
    }
    if (message.timeRange !== undefined) {
      obj.timeRange = TimeRange.toJSON(message.timeRange);
    }
    if (message.view !== 0) {
      obj.view = clusterSummaryViewToJSON(message.view);
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterSummariesRequest>): QueryClusterSummariesRequest {
    return QueryClusterSummariesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterSummariesRequest>): QueryClusterSummariesRequest {
    const message = createBaseQueryClusterSummariesRequest() as any;
    message.project = object.project ?? "";
    message.failureFilter = object.failureFilter ?? "";
    message.orderBy = object.orderBy ?? "";
    message.metrics = object.metrics?.map((e) => e) || [];
    message.timeRange = (object.timeRange !== undefined && object.timeRange !== null)
      ? TimeRange.fromPartial(object.timeRange)
      : undefined;
    message.view = object.view ?? 0;
    return message;
  },
};

function createBaseQueryClusterSummariesResponse(): QueryClusterSummariesResponse {
  return { clusterSummaries: [] };
}

export const QueryClusterSummariesResponse: MessageFns<QueryClusterSummariesResponse> = {
  encode(message: QueryClusterSummariesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.clusterSummaries) {
      ClusterSummary.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterSummariesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterSummariesResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.clusterSummaries.push(ClusterSummary.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterSummariesResponse {
    return {
      clusterSummaries: globalThis.Array.isArray(object?.clusterSummaries)
        ? object.clusterSummaries.map((e: any) => ClusterSummary.fromJSON(e))
        : [],
    };
  },

  toJSON(message: QueryClusterSummariesResponse): unknown {
    const obj: any = {};
    if (message.clusterSummaries?.length) {
      obj.clusterSummaries = message.clusterSummaries.map((e) => ClusterSummary.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterSummariesResponse>): QueryClusterSummariesResponse {
    return QueryClusterSummariesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterSummariesResponse>): QueryClusterSummariesResponse {
    const message = createBaseQueryClusterSummariesResponse() as any;
    message.clusterSummaries = object.clusterSummaries?.map((e) => ClusterSummary.fromPartial(e)) || [];
    return message;
  },
};

function createBaseClusterSummary(): ClusterSummary {
  return { clusterId: undefined, title: "", bug: undefined, metrics: {} };
}

export const ClusterSummary: MessageFns<ClusterSummary> = {
  encode(message: ClusterSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.clusterId !== undefined) {
      ClusterId.encode(message.clusterId, writer.uint32(10).fork()).join();
    }
    if (message.title !== "") {
      writer.uint32(18).string(message.title);
    }
    if (message.bug !== undefined) {
      AssociatedBug.encode(message.bug, writer.uint32(26).fork()).join();
    }
    Object.entries(message.metrics).forEach(([key, value]) => {
      ClusterSummary_MetricsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterSummary() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.clusterId = ClusterId.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.title = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.bug = AssociatedBug.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          const entry7 = ClusterSummary_MetricsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.metrics[entry7.key] = entry7.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterSummary {
    return {
      clusterId: isSet(object.clusterId) ? ClusterId.fromJSON(object.clusterId) : undefined,
      title: isSet(object.title) ? globalThis.String(object.title) : "",
      bug: isSet(object.bug) ? AssociatedBug.fromJSON(object.bug) : undefined,
      metrics: isObject(object.metrics)
        ? Object.entries(object.metrics).reduce<{ [key: string]: ClusterSummary_MetricValue }>((acc, [key, value]) => {
          acc[key] = ClusterSummary_MetricValue.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ClusterSummary): unknown {
    const obj: any = {};
    if (message.clusterId !== undefined) {
      obj.clusterId = ClusterId.toJSON(message.clusterId);
    }
    if (message.title !== "") {
      obj.title = message.title;
    }
    if (message.bug !== undefined) {
      obj.bug = AssociatedBug.toJSON(message.bug);
    }
    if (message.metrics) {
      const entries = Object.entries(message.metrics);
      if (entries.length > 0) {
        obj.metrics = {};
        entries.forEach(([k, v]) => {
          obj.metrics[k] = ClusterSummary_MetricValue.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterSummary>): ClusterSummary {
    return ClusterSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterSummary>): ClusterSummary {
    const message = createBaseClusterSummary() as any;
    message.clusterId = (object.clusterId !== undefined && object.clusterId !== null)
      ? ClusterId.fromPartial(object.clusterId)
      : undefined;
    message.title = object.title ?? "";
    message.bug = (object.bug !== undefined && object.bug !== null) ? AssociatedBug.fromPartial(object.bug) : undefined;
    message.metrics = Object.entries(object.metrics ?? {}).reduce<{ [key: string]: ClusterSummary_MetricValue }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = ClusterSummary_MetricValue.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseClusterSummary_MetricValue(): ClusterSummary_MetricValue {
  return { value: "0", dailyBreakdown: [] };
}

export const ClusterSummary_MetricValue: MessageFns<ClusterSummary_MetricValue> = {
  encode(message: ClusterSummary_MetricValue, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.value !== "0") {
      writer.uint32(8).int64(message.value);
    }
    writer.uint32(18).fork();
    for (const v of message.dailyBreakdown) {
      writer.int64(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterSummary_MetricValue {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterSummary_MetricValue() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.value = reader.int64().toString();
          continue;
        }
        case 2: {
          if (tag === 16) {
            message.dailyBreakdown.push(reader.int64().toString());

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.dailyBreakdown.push(reader.int64().toString());
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterSummary_MetricValue {
    return {
      value: isSet(object.value) ? globalThis.String(object.value) : "0",
      dailyBreakdown: globalThis.Array.isArray(object?.dailyBreakdown)
        ? object.dailyBreakdown.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ClusterSummary_MetricValue): unknown {
    const obj: any = {};
    if (message.value !== "0") {
      obj.value = message.value;
    }
    if (message.dailyBreakdown?.length) {
      obj.dailyBreakdown = message.dailyBreakdown;
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterSummary_MetricValue>): ClusterSummary_MetricValue {
    return ClusterSummary_MetricValue.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterSummary_MetricValue>): ClusterSummary_MetricValue {
    const message = createBaseClusterSummary_MetricValue() as any;
    message.value = object.value ?? "0";
    message.dailyBreakdown = object.dailyBreakdown?.map((e) => e) || [];
    return message;
  },
};

function createBaseClusterSummary_MetricsEntry(): ClusterSummary_MetricsEntry {
  return { key: "", value: undefined };
}

export const ClusterSummary_MetricsEntry: MessageFns<ClusterSummary_MetricsEntry> = {
  encode(message: ClusterSummary_MetricsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ClusterSummary_MetricValue.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterSummary_MetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterSummary_MetricsEntry() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = ClusterSummary_MetricValue.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterSummary_MetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ClusterSummary_MetricValue.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ClusterSummary_MetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ClusterSummary_MetricValue.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterSummary_MetricsEntry>): ClusterSummary_MetricsEntry {
    return ClusterSummary_MetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterSummary_MetricsEntry>): ClusterSummary_MetricsEntry {
    const message = createBaseClusterSummary_MetricsEntry() as any;
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ClusterSummary_MetricValue.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseQueryClusterFailuresRequest(): QueryClusterFailuresRequest {
  return { parent: "", metricFilter: "" };
}

export const QueryClusterFailuresRequest: MessageFns<QueryClusterFailuresRequest> = {
  encode(message: QueryClusterFailuresRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.metricFilter !== "") {
      writer.uint32(18).string(message.metricFilter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterFailuresRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterFailuresRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.metricFilter = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterFailuresRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      metricFilter: isSet(object.metricFilter) ? globalThis.String(object.metricFilter) : "",
    };
  },

  toJSON(message: QueryClusterFailuresRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.metricFilter !== "") {
      obj.metricFilter = message.metricFilter;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterFailuresRequest>): QueryClusterFailuresRequest {
    return QueryClusterFailuresRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterFailuresRequest>): QueryClusterFailuresRequest {
    const message = createBaseQueryClusterFailuresRequest() as any;
    message.parent = object.parent ?? "";
    message.metricFilter = object.metricFilter ?? "";
    return message;
  },
};

function createBaseQueryClusterFailuresResponse(): QueryClusterFailuresResponse {
  return { failures: [] };
}

export const QueryClusterFailuresResponse: MessageFns<QueryClusterFailuresResponse> = {
  encode(message: QueryClusterFailuresResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.failures) {
      DistinctClusterFailure.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterFailuresResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterFailuresResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.failures.push(DistinctClusterFailure.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterFailuresResponse {
    return {
      failures: globalThis.Array.isArray(object?.failures)
        ? object.failures.map((e: any) => DistinctClusterFailure.fromJSON(e))
        : [],
    };
  },

  toJSON(message: QueryClusterFailuresResponse): unknown {
    const obj: any = {};
    if (message.failures?.length) {
      obj.failures = message.failures.map((e) => DistinctClusterFailure.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterFailuresResponse>): QueryClusterFailuresResponse {
    return QueryClusterFailuresResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterFailuresResponse>): QueryClusterFailuresResponse {
    const message = createBaseQueryClusterFailuresResponse() as any;
    message.failures = object.failures?.map((e) => DistinctClusterFailure.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDistinctClusterFailure(): DistinctClusterFailure {
  return {
    testId: "",
    variant: undefined,
    partitionTime: undefined,
    presubmitRun: undefined,
    isBuildCritical: false,
    exonerations: [],
    buildStatus: 0,
    ingestedInvocationId: "",
    isIngestedInvocationBlocked: false,
    changelists: [],
    count: 0,
    failureReasonPrefix: "",
  };
}

export const DistinctClusterFailure: MessageFns<DistinctClusterFailure> = {
  encode(message: DistinctClusterFailure, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.testId !== "") {
      writer.uint32(10).string(message.testId);
    }
    if (message.variant !== undefined) {
      Variant.encode(message.variant, writer.uint32(18).fork()).join();
    }
    if (message.partitionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.partitionTime), writer.uint32(26).fork()).join();
    }
    if (message.presubmitRun !== undefined) {
      DistinctClusterFailure_PresubmitRun.encode(message.presubmitRun, writer.uint32(34).fork()).join();
    }
    if (message.isBuildCritical !== false) {
      writer.uint32(40).bool(message.isBuildCritical);
    }
    for (const v of message.exonerations) {
      DistinctClusterFailure_Exoneration.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.buildStatus !== 0) {
      writer.uint32(56).int32(message.buildStatus);
    }
    if (message.ingestedInvocationId !== "") {
      writer.uint32(66).string(message.ingestedInvocationId);
    }
    if (message.isIngestedInvocationBlocked !== false) {
      writer.uint32(72).bool(message.isIngestedInvocationBlocked);
    }
    for (const v of message.changelists) {
      Changelist.encode(v!, writer.uint32(82).fork()).join();
    }
    if (message.count !== 0) {
      writer.uint32(88).int32(message.count);
    }
    if (message.failureReasonPrefix !== "") {
      writer.uint32(98).string(message.failureReasonPrefix);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DistinctClusterFailure {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistinctClusterFailure() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.testId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.variant = Variant.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.partitionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.presubmitRun = DistinctClusterFailure_PresubmitRun.decode(reader, reader.uint32());
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.isBuildCritical = reader.bool();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.exonerations.push(DistinctClusterFailure_Exoneration.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.buildStatus = reader.int32() as any;
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.ingestedInvocationId = reader.string();
          continue;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.isIngestedInvocationBlocked = reader.bool();
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.changelists.push(Changelist.decode(reader, reader.uint32()));
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.count = reader.int32();
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.failureReasonPrefix = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DistinctClusterFailure {
    return {
      testId: isSet(object.testId) ? globalThis.String(object.testId) : "",
      variant: isSet(object.variant) ? Variant.fromJSON(object.variant) : undefined,
      partitionTime: isSet(object.partitionTime) ? globalThis.String(object.partitionTime) : undefined,
      presubmitRun: isSet(object.presubmitRun)
        ? DistinctClusterFailure_PresubmitRun.fromJSON(object.presubmitRun)
        : undefined,
      isBuildCritical: isSet(object.isBuildCritical) ? globalThis.Boolean(object.isBuildCritical) : false,
      exonerations: globalThis.Array.isArray(object?.exonerations)
        ? object.exonerations.map((e: any) => DistinctClusterFailure_Exoneration.fromJSON(e))
        : [],
      buildStatus: isSet(object.buildStatus) ? buildStatusFromJSON(object.buildStatus) : 0,
      ingestedInvocationId: isSet(object.ingestedInvocationId) ? globalThis.String(object.ingestedInvocationId) : "",
      isIngestedInvocationBlocked: isSet(object.isIngestedInvocationBlocked)
        ? globalThis.Boolean(object.isIngestedInvocationBlocked)
        : false,
      changelists: globalThis.Array.isArray(object?.changelists)
        ? object.changelists.map((e: any) => Changelist.fromJSON(e))
        : [],
      count: isSet(object.count) ? globalThis.Number(object.count) : 0,
      failureReasonPrefix: isSet(object.failureReasonPrefix) ? globalThis.String(object.failureReasonPrefix) : "",
    };
  },

  toJSON(message: DistinctClusterFailure): unknown {
    const obj: any = {};
    if (message.testId !== "") {
      obj.testId = message.testId;
    }
    if (message.variant !== undefined) {
      obj.variant = Variant.toJSON(message.variant);
    }
    if (message.partitionTime !== undefined) {
      obj.partitionTime = message.partitionTime;
    }
    if (message.presubmitRun !== undefined) {
      obj.presubmitRun = DistinctClusterFailure_PresubmitRun.toJSON(message.presubmitRun);
    }
    if (message.isBuildCritical !== false) {
      obj.isBuildCritical = message.isBuildCritical;
    }
    if (message.exonerations?.length) {
      obj.exonerations = message.exonerations.map((e) => DistinctClusterFailure_Exoneration.toJSON(e));
    }
    if (message.buildStatus !== 0) {
      obj.buildStatus = buildStatusToJSON(message.buildStatus);
    }
    if (message.ingestedInvocationId !== "") {
      obj.ingestedInvocationId = message.ingestedInvocationId;
    }
    if (message.isIngestedInvocationBlocked !== false) {
      obj.isIngestedInvocationBlocked = message.isIngestedInvocationBlocked;
    }
    if (message.changelists?.length) {
      obj.changelists = message.changelists.map((e) => Changelist.toJSON(e));
    }
    if (message.count !== 0) {
      obj.count = Math.round(message.count);
    }
    if (message.failureReasonPrefix !== "") {
      obj.failureReasonPrefix = message.failureReasonPrefix;
    }
    return obj;
  },

  create(base?: DeepPartial<DistinctClusterFailure>): DistinctClusterFailure {
    return DistinctClusterFailure.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DistinctClusterFailure>): DistinctClusterFailure {
    const message = createBaseDistinctClusterFailure() as any;
    message.testId = object.testId ?? "";
    message.variant = (object.variant !== undefined && object.variant !== null)
      ? Variant.fromPartial(object.variant)
      : undefined;
    message.partitionTime = object.partitionTime ?? undefined;
    message.presubmitRun = (object.presubmitRun !== undefined && object.presubmitRun !== null)
      ? DistinctClusterFailure_PresubmitRun.fromPartial(object.presubmitRun)
      : undefined;
    message.isBuildCritical = object.isBuildCritical ?? false;
    message.exonerations = object.exonerations?.map((e) => DistinctClusterFailure_Exoneration.fromPartial(e)) || [];
    message.buildStatus = object.buildStatus ?? 0;
    message.ingestedInvocationId = object.ingestedInvocationId ?? "";
    message.isIngestedInvocationBlocked = object.isIngestedInvocationBlocked ?? false;
    message.changelists = object.changelists?.map((e) => Changelist.fromPartial(e)) || [];
    message.count = object.count ?? 0;
    message.failureReasonPrefix = object.failureReasonPrefix ?? "";
    return message;
  },
};

function createBaseDistinctClusterFailure_Exoneration(): DistinctClusterFailure_Exoneration {
  return { reason: 0 };
}

export const DistinctClusterFailure_Exoneration: MessageFns<DistinctClusterFailure_Exoneration> = {
  encode(message: DistinctClusterFailure_Exoneration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.reason !== 0) {
      writer.uint32(8).int32(message.reason);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DistinctClusterFailure_Exoneration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistinctClusterFailure_Exoneration() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.reason = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DistinctClusterFailure_Exoneration {
    return { reason: isSet(object.reason) ? exonerationReasonFromJSON(object.reason) : 0 };
  },

  toJSON(message: DistinctClusterFailure_Exoneration): unknown {
    const obj: any = {};
    if (message.reason !== 0) {
      obj.reason = exonerationReasonToJSON(message.reason);
    }
    return obj;
  },

  create(base?: DeepPartial<DistinctClusterFailure_Exoneration>): DistinctClusterFailure_Exoneration {
    return DistinctClusterFailure_Exoneration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DistinctClusterFailure_Exoneration>): DistinctClusterFailure_Exoneration {
    const message = createBaseDistinctClusterFailure_Exoneration() as any;
    message.reason = object.reason ?? 0;
    return message;
  },
};

function createBaseDistinctClusterFailure_PresubmitRun(): DistinctClusterFailure_PresubmitRun {
  return { presubmitRunId: undefined, owner: "", mode: 0, status: 0 };
}

export const DistinctClusterFailure_PresubmitRun: MessageFns<DistinctClusterFailure_PresubmitRun> = {
  encode(message: DistinctClusterFailure_PresubmitRun, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.presubmitRunId !== undefined) {
      PresubmitRunId.encode(message.presubmitRunId, writer.uint32(10).fork()).join();
    }
    if (message.owner !== "") {
      writer.uint32(18).string(message.owner);
    }
    if (message.mode !== 0) {
      writer.uint32(24).int32(message.mode);
    }
    if (message.status !== 0) {
      writer.uint32(32).int32(message.status);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DistinctClusterFailure_PresubmitRun {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistinctClusterFailure_PresubmitRun() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.presubmitRunId = PresubmitRunId.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.owner = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DistinctClusterFailure_PresubmitRun {
    return {
      presubmitRunId: isSet(object.presubmitRunId) ? PresubmitRunId.fromJSON(object.presubmitRunId) : undefined,
      owner: isSet(object.owner) ? globalThis.String(object.owner) : "",
      mode: isSet(object.mode) ? presubmitRunModeFromJSON(object.mode) : 0,
      status: isSet(object.status) ? presubmitRunStatusFromJSON(object.status) : 0,
    };
  },

  toJSON(message: DistinctClusterFailure_PresubmitRun): unknown {
    const obj: any = {};
    if (message.presubmitRunId !== undefined) {
      obj.presubmitRunId = PresubmitRunId.toJSON(message.presubmitRunId);
    }
    if (message.owner !== "") {
      obj.owner = message.owner;
    }
    if (message.mode !== 0) {
      obj.mode = presubmitRunModeToJSON(message.mode);
    }
    if (message.status !== 0) {
      obj.status = presubmitRunStatusToJSON(message.status);
    }
    return obj;
  },

  create(base?: DeepPartial<DistinctClusterFailure_PresubmitRun>): DistinctClusterFailure_PresubmitRun {
    return DistinctClusterFailure_PresubmitRun.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DistinctClusterFailure_PresubmitRun>): DistinctClusterFailure_PresubmitRun {
    const message = createBaseDistinctClusterFailure_PresubmitRun() as any;
    message.presubmitRunId = (object.presubmitRunId !== undefined && object.presubmitRunId !== null)
      ? PresubmitRunId.fromPartial(object.presubmitRunId)
      : undefined;
    message.owner = object.owner ?? "";
    message.mode = object.mode ?? 0;
    message.status = object.status ?? 0;
    return message;
  },
};

function createBaseQueryClusterExoneratedTestVariantsRequest(): QueryClusterExoneratedTestVariantsRequest {
  return { parent: "" };
}

export const QueryClusterExoneratedTestVariantsRequest: MessageFns<QueryClusterExoneratedTestVariantsRequest> = {
  encode(message: QueryClusterExoneratedTestVariantsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterExoneratedTestVariantsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterExoneratedTestVariantsRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterExoneratedTestVariantsRequest {
    return { parent: isSet(object.parent) ? globalThis.String(object.parent) : "" };
  },

  toJSON(message: QueryClusterExoneratedTestVariantsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterExoneratedTestVariantsRequest>): QueryClusterExoneratedTestVariantsRequest {
    return QueryClusterExoneratedTestVariantsRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<QueryClusterExoneratedTestVariantsRequest>,
  ): QueryClusterExoneratedTestVariantsRequest {
    const message = createBaseQueryClusterExoneratedTestVariantsRequest() as any;
    message.parent = object.parent ?? "";
    return message;
  },
};

function createBaseQueryClusterExoneratedTestVariantsResponse(): QueryClusterExoneratedTestVariantsResponse {
  return { testVariants: [] };
}

export const QueryClusterExoneratedTestVariantsResponse: MessageFns<QueryClusterExoneratedTestVariantsResponse> = {
  encode(message: QueryClusterExoneratedTestVariantsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.testVariants) {
      ClusterExoneratedTestVariant.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterExoneratedTestVariantsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterExoneratedTestVariantsResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.testVariants.push(ClusterExoneratedTestVariant.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterExoneratedTestVariantsResponse {
    return {
      testVariants: globalThis.Array.isArray(object?.testVariants)
        ? object.testVariants.map((e: any) => ClusterExoneratedTestVariant.fromJSON(e))
        : [],
    };
  },

  toJSON(message: QueryClusterExoneratedTestVariantsResponse): unknown {
    const obj: any = {};
    if (message.testVariants?.length) {
      obj.testVariants = message.testVariants.map((e) => ClusterExoneratedTestVariant.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterExoneratedTestVariantsResponse>): QueryClusterExoneratedTestVariantsResponse {
    return QueryClusterExoneratedTestVariantsResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<QueryClusterExoneratedTestVariantsResponse>,
  ): QueryClusterExoneratedTestVariantsResponse {
    const message = createBaseQueryClusterExoneratedTestVariantsResponse() as any;
    message.testVariants = object.testVariants?.map((e) => ClusterExoneratedTestVariant.fromPartial(e)) || [];
    return message;
  },
};

function createBaseClusterExoneratedTestVariant(): ClusterExoneratedTestVariant {
  return { testId: "", variant: undefined, criticalFailuresExonerated: 0, lastExoneration: undefined };
}

export const ClusterExoneratedTestVariant: MessageFns<ClusterExoneratedTestVariant> = {
  encode(message: ClusterExoneratedTestVariant, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.testId !== "") {
      writer.uint32(10).string(message.testId);
    }
    if (message.variant !== undefined) {
      Variant.encode(message.variant, writer.uint32(18).fork()).join();
    }
    if (message.criticalFailuresExonerated !== 0) {
      writer.uint32(24).int32(message.criticalFailuresExonerated);
    }
    if (message.lastExoneration !== undefined) {
      Timestamp.encode(toTimestamp(message.lastExoneration), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterExoneratedTestVariant {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterExoneratedTestVariant() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.testId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.variant = Variant.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.criticalFailuresExonerated = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.lastExoneration = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterExoneratedTestVariant {
    return {
      testId: isSet(object.testId) ? globalThis.String(object.testId) : "",
      variant: isSet(object.variant) ? Variant.fromJSON(object.variant) : undefined,
      criticalFailuresExonerated: isSet(object.criticalFailuresExonerated)
        ? globalThis.Number(object.criticalFailuresExonerated)
        : 0,
      lastExoneration: isSet(object.lastExoneration) ? globalThis.String(object.lastExoneration) : undefined,
    };
  },

  toJSON(message: ClusterExoneratedTestVariant): unknown {
    const obj: any = {};
    if (message.testId !== "") {
      obj.testId = message.testId;
    }
    if (message.variant !== undefined) {
      obj.variant = Variant.toJSON(message.variant);
    }
    if (message.criticalFailuresExonerated !== 0) {
      obj.criticalFailuresExonerated = Math.round(message.criticalFailuresExonerated);
    }
    if (message.lastExoneration !== undefined) {
      obj.lastExoneration = message.lastExoneration;
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterExoneratedTestVariant>): ClusterExoneratedTestVariant {
    return ClusterExoneratedTestVariant.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterExoneratedTestVariant>): ClusterExoneratedTestVariant {
    const message = createBaseClusterExoneratedTestVariant() as any;
    message.testId = object.testId ?? "";
    message.variant = (object.variant !== undefined && object.variant !== null)
      ? Variant.fromPartial(object.variant)
      : undefined;
    message.criticalFailuresExonerated = object.criticalFailuresExonerated ?? 0;
    message.lastExoneration = object.lastExoneration ?? undefined;
    return message;
  },
};

function createBaseQueryClusterExoneratedTestVariantBranchesRequest(): QueryClusterExoneratedTestVariantBranchesRequest {
  return { parent: "" };
}

export const QueryClusterExoneratedTestVariantBranchesRequest: MessageFns<
  QueryClusterExoneratedTestVariantBranchesRequest
> = {
  encode(
    message: QueryClusterExoneratedTestVariantBranchesRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterExoneratedTestVariantBranchesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterExoneratedTestVariantBranchesRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterExoneratedTestVariantBranchesRequest {
    return { parent: isSet(object.parent) ? globalThis.String(object.parent) : "" };
  },

  toJSON(message: QueryClusterExoneratedTestVariantBranchesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    return obj;
  },

  create(
    base?: DeepPartial<QueryClusterExoneratedTestVariantBranchesRequest>,
  ): QueryClusterExoneratedTestVariantBranchesRequest {
    return QueryClusterExoneratedTestVariantBranchesRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<QueryClusterExoneratedTestVariantBranchesRequest>,
  ): QueryClusterExoneratedTestVariantBranchesRequest {
    const message = createBaseQueryClusterExoneratedTestVariantBranchesRequest() as any;
    message.parent = object.parent ?? "";
    return message;
  },
};

function createBaseQueryClusterExoneratedTestVariantBranchesResponse(): QueryClusterExoneratedTestVariantBranchesResponse {
  return { testVariantBranches: [] };
}

export const QueryClusterExoneratedTestVariantBranchesResponse: MessageFns<
  QueryClusterExoneratedTestVariantBranchesResponse
> = {
  encode(
    message: QueryClusterExoneratedTestVariantBranchesResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.testVariantBranches) {
      ClusterExoneratedTestVariantBranch.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterExoneratedTestVariantBranchesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterExoneratedTestVariantBranchesResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.testVariantBranches.push(ClusterExoneratedTestVariantBranch.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterExoneratedTestVariantBranchesResponse {
    return {
      testVariantBranches: globalThis.Array.isArray(object?.testVariantBranches)
        ? object.testVariantBranches.map((e: any) => ClusterExoneratedTestVariantBranch.fromJSON(e))
        : [],
    };
  },

  toJSON(message: QueryClusterExoneratedTestVariantBranchesResponse): unknown {
    const obj: any = {};
    if (message.testVariantBranches?.length) {
      obj.testVariantBranches = message.testVariantBranches.map((e) => ClusterExoneratedTestVariantBranch.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<QueryClusterExoneratedTestVariantBranchesResponse>,
  ): QueryClusterExoneratedTestVariantBranchesResponse {
    return QueryClusterExoneratedTestVariantBranchesResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<QueryClusterExoneratedTestVariantBranchesResponse>,
  ): QueryClusterExoneratedTestVariantBranchesResponse {
    const message = createBaseQueryClusterExoneratedTestVariantBranchesResponse() as any;
    message.testVariantBranches =
      object.testVariantBranches?.map((e) => ClusterExoneratedTestVariantBranch.fromPartial(e)) || [];
    return message;
  },
};

function createBaseClusterExoneratedTestVariantBranch(): ClusterExoneratedTestVariantBranch {
  return {
    project: "",
    testId: "",
    variant: undefined,
    sourceRef: undefined,
    criticalFailuresExonerated: 0,
    lastExoneration: undefined,
  };
}

export const ClusterExoneratedTestVariantBranch: MessageFns<ClusterExoneratedTestVariantBranch> = {
  encode(message: ClusterExoneratedTestVariantBranch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.project !== "") {
      writer.uint32(10).string(message.project);
    }
    if (message.testId !== "") {
      writer.uint32(18).string(message.testId);
    }
    if (message.variant !== undefined) {
      Variant.encode(message.variant, writer.uint32(26).fork()).join();
    }
    if (message.sourceRef !== undefined) {
      SourceRef.encode(message.sourceRef, writer.uint32(34).fork()).join();
    }
    if (message.criticalFailuresExonerated !== 0) {
      writer.uint32(40).int32(message.criticalFailuresExonerated);
    }
    if (message.lastExoneration !== undefined) {
      Timestamp.encode(toTimestamp(message.lastExoneration), writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterExoneratedTestVariantBranch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterExoneratedTestVariantBranch() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.project = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.testId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.variant = Variant.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.sourceRef = SourceRef.decode(reader, reader.uint32());
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.criticalFailuresExonerated = reader.int32();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.lastExoneration = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterExoneratedTestVariantBranch {
    return {
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      testId: isSet(object.testId) ? globalThis.String(object.testId) : "",
      variant: isSet(object.variant) ? Variant.fromJSON(object.variant) : undefined,
      sourceRef: isSet(object.sourceRef) ? SourceRef.fromJSON(object.sourceRef) : undefined,
      criticalFailuresExonerated: isSet(object.criticalFailuresExonerated)
        ? globalThis.Number(object.criticalFailuresExonerated)
        : 0,
      lastExoneration: isSet(object.lastExoneration) ? globalThis.String(object.lastExoneration) : undefined,
    };
  },

  toJSON(message: ClusterExoneratedTestVariantBranch): unknown {
    const obj: any = {};
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.testId !== "") {
      obj.testId = message.testId;
    }
    if (message.variant !== undefined) {
      obj.variant = Variant.toJSON(message.variant);
    }
    if (message.sourceRef !== undefined) {
      obj.sourceRef = SourceRef.toJSON(message.sourceRef);
    }
    if (message.criticalFailuresExonerated !== 0) {
      obj.criticalFailuresExonerated = Math.round(message.criticalFailuresExonerated);
    }
    if (message.lastExoneration !== undefined) {
      obj.lastExoneration = message.lastExoneration;
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterExoneratedTestVariantBranch>): ClusterExoneratedTestVariantBranch {
    return ClusterExoneratedTestVariantBranch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterExoneratedTestVariantBranch>): ClusterExoneratedTestVariantBranch {
    const message = createBaseClusterExoneratedTestVariantBranch() as any;
    message.project = object.project ?? "";
    message.testId = object.testId ?? "";
    message.variant = (object.variant !== undefined && object.variant !== null)
      ? Variant.fromPartial(object.variant)
      : undefined;
    message.sourceRef = (object.sourceRef !== undefined && object.sourceRef !== null)
      ? SourceRef.fromPartial(object.sourceRef)
      : undefined;
    message.criticalFailuresExonerated = object.criticalFailuresExonerated ?? 0;
    message.lastExoneration = object.lastExoneration ?? undefined;
    return message;
  },
};

function createBaseQueryClusterHistoryRequest(): QueryClusterHistoryRequest {
  return { project: "", failureFilter: "", days: 0, metrics: [] };
}

export const QueryClusterHistoryRequest: MessageFns<QueryClusterHistoryRequest> = {
  encode(message: QueryClusterHistoryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.project !== "") {
      writer.uint32(10).string(message.project);
    }
    if (message.failureFilter !== "") {
      writer.uint32(18).string(message.failureFilter);
    }
    if (message.days !== 0) {
      writer.uint32(24).int32(message.days);
    }
    for (const v of message.metrics) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterHistoryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterHistoryRequest() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.project = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.failureFilter = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.days = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.metrics.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterHistoryRequest {
    return {
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      failureFilter: isSet(object.failureFilter) ? globalThis.String(object.failureFilter) : "",
      days: isSet(object.days) ? globalThis.Number(object.days) : 0,
      metrics: globalThis.Array.isArray(object?.metrics) ? object.metrics.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: QueryClusterHistoryRequest): unknown {
    const obj: any = {};
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.failureFilter !== "") {
      obj.failureFilter = message.failureFilter;
    }
    if (message.days !== 0) {
      obj.days = Math.round(message.days);
    }
    if (message.metrics?.length) {
      obj.metrics = message.metrics;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterHistoryRequest>): QueryClusterHistoryRequest {
    return QueryClusterHistoryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterHistoryRequest>): QueryClusterHistoryRequest {
    const message = createBaseQueryClusterHistoryRequest() as any;
    message.project = object.project ?? "";
    message.failureFilter = object.failureFilter ?? "";
    message.days = object.days ?? 0;
    message.metrics = object.metrics?.map((e) => e) || [];
    return message;
  },
};

function createBaseQueryClusterHistoryResponse(): QueryClusterHistoryResponse {
  return { days: [] };
}

export const QueryClusterHistoryResponse: MessageFns<QueryClusterHistoryResponse> = {
  encode(message: QueryClusterHistoryResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.days) {
      ClusterHistoryDay.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryClusterHistoryResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryClusterHistoryResponse() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.days.push(ClusterHistoryDay.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryClusterHistoryResponse {
    return {
      days: globalThis.Array.isArray(object?.days) ? object.days.map((e: any) => ClusterHistoryDay.fromJSON(e)) : [],
    };
  },

  toJSON(message: QueryClusterHistoryResponse): unknown {
    const obj: any = {};
    if (message.days?.length) {
      obj.days = message.days.map((e) => ClusterHistoryDay.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<QueryClusterHistoryResponse>): QueryClusterHistoryResponse {
    return QueryClusterHistoryResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryClusterHistoryResponse>): QueryClusterHistoryResponse {
    const message = createBaseQueryClusterHistoryResponse() as any;
    message.days = object.days?.map((e) => ClusterHistoryDay.fromPartial(e)) || [];
    return message;
  },
};

function createBaseClusterHistoryDay(): ClusterHistoryDay {
  return { metrics: {}, date: "" };
}

export const ClusterHistoryDay: MessageFns<ClusterHistoryDay> = {
  encode(message: ClusterHistoryDay, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.metrics).forEach(([key, value]) => {
      ClusterHistoryDay_MetricsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    if (message.date !== "") {
      writer.uint32(18).string(message.date);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterHistoryDay {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterHistoryDay() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          const entry1 = ClusterHistoryDay_MetricsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.metrics[entry1.key] = entry1.value;
          }
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.date = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterHistoryDay {
    return {
      metrics: isObject(object.metrics)
        ? Object.entries(object.metrics).reduce<{ [key: string]: number }>((acc, [key, value]) => {
          acc[key] = Number(value);
          return acc;
        }, {})
        : {},
      date: isSet(object.date) ? globalThis.String(object.date) : "",
    };
  },

  toJSON(message: ClusterHistoryDay): unknown {
    const obj: any = {};
    if (message.metrics) {
      const entries = Object.entries(message.metrics);
      if (entries.length > 0) {
        obj.metrics = {};
        entries.forEach(([k, v]) => {
          obj.metrics[k] = Math.round(v);
        });
      }
    }
    if (message.date !== "") {
      obj.date = message.date;
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterHistoryDay>): ClusterHistoryDay {
    return ClusterHistoryDay.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterHistoryDay>): ClusterHistoryDay {
    const message = createBaseClusterHistoryDay() as any;
    message.metrics = Object.entries(object.metrics ?? {}).reduce<{ [key: string]: number }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.Number(value);
      }
      return acc;
    }, {});
    message.date = object.date ?? "";
    return message;
  },
};

function createBaseClusterHistoryDay_MetricsEntry(): ClusterHistoryDay_MetricsEntry {
  return { key: "", value: 0 };
}

export const ClusterHistoryDay_MetricsEntry: MessageFns<ClusterHistoryDay_MetricsEntry> = {
  encode(message: ClusterHistoryDay_MetricsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== 0) {
      writer.uint32(16).int32(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterHistoryDay_MetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterHistoryDay_MetricsEntry() as any;
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.value = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterHistoryDay_MetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.Number(object.value) : 0,
    };
  },

  toJSON(message: ClusterHistoryDay_MetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== 0) {
      obj.value = Math.round(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterHistoryDay_MetricsEntry>): ClusterHistoryDay_MetricsEntry {
    return ClusterHistoryDay_MetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterHistoryDay_MetricsEntry>): ClusterHistoryDay_MetricsEntry {
    const message = createBaseClusterHistoryDay_MetricsEntry() as any;
    message.key = object.key ?? "";
    message.value = object.value ?? 0;
    return message;
  },
};

/**
 * Provides methods to cluster test results, and obtain the impact of those
 * clusters.
 *
 * A cluster is a group of test failures with a common characteristic.
 * For example, test results may form a cluster with other failures that share
 * a common test name, or failure reason. Test results may also be in a cluster
 * defined by a user-modifiable failure association rule (which associates
 * failures with a bug). In this case, the failures have the property defined
 * by the failure association rule in common.
 *
 * A test result may be in many clusters, and each cluster may contain many
 * test results.
 *
 * Each cluster has an identity, consisting of three components:
 * - The LUCI Project name, e.g. "chromium" or "fuchsia".
 * - The Clustering Algorithm that identified the cluster. As at writing
 *   (April 2022), the algorithms are 'testname-v3' for version 3 of the
 *   test-name clustering algorithm, 'reason-v3' for version 3 of the failure
 *   reason clustering algorithm, and 'rules-v2' for the rules-based clustering
 *   algorithm.
 *   (Although internally versioned, the rules algorithm version is hidden
 *   for clients, so that {luci_project}/rules/{rule_id} always represents
 *   the cluster defined by the given rule_id.)
 *   We make no guarantees about the structure of algorithm names, they should
 *   be treated as opaque strings by clients.
 * - An algorithm-defined cluster identifier. This is algorithm-dependent and
 *   although (as at April 2022) a lowercase hexadecimal string, should be
 *   treated as an opaque value by clients.
 *   For the 'rules' algorithm, the cluster identifier will always correspond
 *   to the Rule ID of the rule that defines the cluster.
 */
export interface Clusters {
  /**
   * Identifies the cluster(s) for one or more test failure(s).
   *
   * This RPC returns the clusters of each test result, using
   * current suggested cluster algorithms, configured failure
   * association rules, and ingested project configuration with
   * a bounded staleness of up to one minute. (Returned clusters
   * may be based on project configuration and configured failure
   * association rules that is up to one minute out-of-date).
   *
   * As at April 2022, the implementation does not use stale
   * rules, but you are instructed NOT to rely on this property to
   * allow reversion to the faster implementation that is tolerant
   * to higher QPS in future. If your use case require strong reads
   * (e.g. you want to call cluster immediately after updating a rule),
   * please contact LUCI Analysis owners. We may be able to provide a
   * request flag to select this processing behaviour.
   *
   * This RPC is a pure query API and does not lead to the ingestion of the
   * test failures by LUCI Analysis (e.g. for cluster impact calculations).
   */
  Cluster(request: ClusterRequest): Promise<ClusterResponse>;
  /**
   * Reads information about the given cluster.
   *
   * Please consult LUCI Analysis owners before adding additional calls to
   * this RPC, as the implementation currently calls back to BigQuery and as
   * such, is not cost-optimised if many queries are to be made.
   *
   * As of writing (April 13, 2022) this query reads ~1 GB per call for
   * the largest LUCI Project, which translates to a cost of 0.5 US cents
   * per query at published pricing (US$5/TB analyzed for BigQuery).
   *
   * Changes to this RPC should comply with https://google.aip.dev/131.
   */
  Get(request: GetClusterRequest): Promise<Cluster>;
  /**
   * Reads current progress re-clustering the given project. Re-clustering
   * means updating the clusters each failure is in to reflect the latest
   * failure association rules, suggested clustering algorithms and
   * clustering configuration.
   */
  GetReclusteringProgress(request: GetReclusteringProgressRequest): Promise<ReclusteringProgress>;
  /**
   * Queries summary information about top clusters.
   *
   * The set of test failures used as input to the clustering can be
   * specified using the failure_filter field on the request.
   * The returned clusters include only the impact derived from the
   * filtered failures.
   *
   * This allows investigation of the highest impact clusters for some
   * subset of the failure data in a project. For example, a filter string
   * of "failure_reason:ssh" would find all of the clusters where any test
   * results mention "ssh" in their failure reason, and show how big the
   * impact from these ssh failures is in each cluster. This is useful when
   * investigating specific problems, or ownership areas of the tests.
   *
   * Please consult LUCI Analysis owners before adding additional calls
   * to this RPC, as the implementation currently calls back to BigQuery and as
   * such, is not cost-optimised if many queries are to be made.
   *
   * As of writing (April 13, 2022) this query reads up to 10 GB per call for
   * 7 days of data for the largest LUCI Project, which translates to a cost
   * of up to 5 US cents per query at published pricing
   * (US$5/TB analyzed for BigQuery).
   */
  QueryClusterSummaries(request: QueryClusterSummariesRequest): Promise<QueryClusterSummariesResponse>;
  /**
   * Queries examples of failures in the given cluster.
   *
   * Please consult LUCI Analysis owners before adding additional calls to
   * this RPC, as the implementation currently calls back to BigQuery and as
   * such, is not cost-optimised if many queries are to be made.
   */
  QueryClusterFailures(request: QueryClusterFailuresRequest): Promise<QueryClusterFailuresResponse>;
  /**
   * Queries test variants in the cluster which have recently had an
   * exoneration recorded against them. Only exonerations on failures
   * which are part of the cluster are considered.
   *
   * Consider solving this use case in future by a standard AIP-132 List
   * method with filter and order_by support.
   *
   * This RPC is useful for projects using the legacy QueryFailureRate
   * API for exoneration.
   */
  QueryExoneratedTestVariants(
    request: QueryClusterExoneratedTestVariantsRequest,
  ): Promise<QueryClusterExoneratedTestVariantsResponse>;
  /**
   * Queries test variant branches in the cluster which have recently had
   * an exoneration recorded against them. Only exonerations on failures
   * which are part of the cluster are considered.
   *
   * Use for projects performing branch-scoped exoneration using
   * QueryStability.
   */
  QueryExoneratedTestVariantBranches(
    request: QueryClusterExoneratedTestVariantBranchesRequest,
  ): Promise<QueryClusterExoneratedTestVariantBranchesResponse>;
  /**
   * Queries the history of metrics for clustered failures satisying given criteria.
   * For example the number of test runs failed on each day for the last 7 days.
   *
   * Please consult LUCI Analysis owners before adding additional calls to
   * this RPC, as the implementation currently calls back to BigQuery and as
   * such, is not cost-optimised if many queries are to be made.
   */
  QueryHistory(request: QueryClusterHistoryRequest): Promise<QueryClusterHistoryResponse>;
}

export const ClustersServiceName = "luci.analysis.v1.Clusters";
export class ClustersClientImpl implements Clusters {
  static readonly DEFAULT_SERVICE = ClustersServiceName;
  private readonly rpc: Rpc;
  private readonly service: string;
  constructor(rpc: Rpc, opts?: { service?: string }) {
    this.service = opts?.service || ClustersServiceName;
    this.rpc = rpc;
    this.Cluster = this.Cluster.bind(this);
    this.Get = this.Get.bind(this);
    this.GetReclusteringProgress = this.GetReclusteringProgress.bind(this);
    this.QueryClusterSummaries = this.QueryClusterSummaries.bind(this);
    this.QueryClusterFailures = this.QueryClusterFailures.bind(this);
    this.QueryExoneratedTestVariants = this.QueryExoneratedTestVariants.bind(this);
    this.QueryExoneratedTestVariantBranches = this.QueryExoneratedTestVariantBranches.bind(this);
    this.QueryHistory = this.QueryHistory.bind(this);
  }
  Cluster(request: ClusterRequest): Promise<ClusterResponse> {
    const data = ClusterRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "Cluster", data);
    return promise.then((data) => ClusterResponse.fromJSON(data));
  }

  Get(request: GetClusterRequest): Promise<Cluster> {
    const data = GetClusterRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "Get", data);
    return promise.then((data) => Cluster.fromJSON(data));
  }

  GetReclusteringProgress(request: GetReclusteringProgressRequest): Promise<ReclusteringProgress> {
    const data = GetReclusteringProgressRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "GetReclusteringProgress", data);
    return promise.then((data) => ReclusteringProgress.fromJSON(data));
  }

  QueryClusterSummaries(request: QueryClusterSummariesRequest): Promise<QueryClusterSummariesResponse> {
    const data = QueryClusterSummariesRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "QueryClusterSummaries", data);
    return promise.then((data) => QueryClusterSummariesResponse.fromJSON(data));
  }

  QueryClusterFailures(request: QueryClusterFailuresRequest): Promise<QueryClusterFailuresResponse> {
    const data = QueryClusterFailuresRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "QueryClusterFailures", data);
    return promise.then((data) => QueryClusterFailuresResponse.fromJSON(data));
  }

  QueryExoneratedTestVariants(
    request: QueryClusterExoneratedTestVariantsRequest,
  ): Promise<QueryClusterExoneratedTestVariantsResponse> {
    const data = QueryClusterExoneratedTestVariantsRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "QueryExoneratedTestVariants", data);
    return promise.then((data) => QueryClusterExoneratedTestVariantsResponse.fromJSON(data));
  }

  QueryExoneratedTestVariantBranches(
    request: QueryClusterExoneratedTestVariantBranchesRequest,
  ): Promise<QueryClusterExoneratedTestVariantBranchesResponse> {
    const data = QueryClusterExoneratedTestVariantBranchesRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "QueryExoneratedTestVariantBranches", data);
    return promise.then((data) => QueryClusterExoneratedTestVariantBranchesResponse.fromJSON(data));
  }

  QueryHistory(request: QueryClusterHistoryRequest): Promise<QueryClusterHistoryResponse> {
    const data = QueryClusterHistoryRequest.toJSON(request);
    const promise = this.rpc.request(this.service, "QueryHistory", data);
    return promise.then((data) => QueryClusterHistoryResponse.fromJSON(data));
  }
}

interface Rpc {
  request(service: string, method: string, data: unknown): Promise<unknown>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(dateStr: string): Timestamp {
  const date = new globalThis.Date(dateStr);
  const seconds = Math.trunc(date.getTime() / 1_000).toString();
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): string {
  let millis = (globalThis.Number(t.seconds) || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis).toISOString();
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
