// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.5
//   protoc               v6.32.1
// source: turboci/graph/orchestrator/v1/stage_attempt_state.proto

/* eslint-disable */

export const protobufPackage = "turboci.graph.orchestrator.v1";

/**
 * StageAttemptState describes the current state of a StageAttempt.
 *
 * This state machine is much more complicated than Check or Stage state
 * because:
 *   * It revolves around synchronous / asynchronous state reconciliation
 *     with external Executors.
 *   * It subsumes a major portion of the historical WorkNode state machine in
 *     a way that will allow us to be compatible with WorkNode state semantics.
 *
 * This state evolves like:
 *
 *   PENDING -> THROTTLED
 *   PENDING -> RUNNING
 *   PENDING -> COMPLETE
 *   PENDING -> INCOMPLETE
 *   PENDING -> SCHEDULED
 *
 *   THROTTLED -> INCOMPLETE
 *   THROTTLED -> PENDING
 *
 *   SCHEDULED -> RUNNING
 *   SCHEDULED -> COMPLETE
 *   SCHEDULED -> INCOMPLETE
 *
 *   RUNNING -> CANCELLING
 *   RUNNING -> TEARING_DOWN
 *   RUNNING -> COMPLETE
 *   RUNNING -> INCOMPLETE
 *
 *   CANCELLING -> TEARING_DOWN
 *   CANCELLING -> INCOMPLETE
 *
 *   TEARING_DOWN -> COMPLETE
 *   TEARING_DOWN -> INCOMPLETE
 *
 *   INCOMPLETE and COMPLETE are terminal states.
 *
 * The Nth StageAttempt (i.e. any StageAttempt after the first) may also be
 * created in an AWAITING_RETRY state rather than PENDING.
 *
 *   AWAITING_RETRY -> PENDING
 *   AWAITING_RETRY -> INCOMPLETE
 *
 * The Orchestrator manages StageAttempt state evolution in conjunction with the
 * Executor, and explicit state transitions made explicitly by the StageAttempt
 * itself. The only state transition which can be activated by a third party
 * would be transitions to INCOMPLETE or CANCELLING done when the Stage is
 * cancelled.
 *
 * These states have enum values in multiples of 10 in case we need to add more
 * states later which fall between these initial states.
 */
export enum StageAttemptState {
  /** STAGE_ATTEMPT_STATE_UNKNOWN - UNKNOWN is the default, invalid, state. */
  STAGE_ATTEMPT_STATE_UNKNOWN = 0,
  /**
   * STAGE_ATTEMPT_STATE_PENDING - This is an initial state for a StageAttempt and indicates that the
   * StageAttempt is available for an Executor, but either:
   *   * The Orchestrator has not yet sent this StageAttempt to the Executor.
   *   * [WorkNode only] The Executor has advertised this Stage on pub/sub, but
   *     it has not yet been "Popped".
   *
   * This state can transition to THROTTLED, SCHEDULED, RUNNING, COMPLETE or
   * INCOMPLETE.
   */
  STAGE_ATTEMPT_STATE_PENDING = 10,
  /**
   * STAGE_ATTEMPT_STATE_SCHEDULED - This state indicates that the StageAttempt was PENDING, but is now picked
   * up for execution by an Executor.
   *
   * This state can transition to RUNNING, COMPLETE or INCOMPLETE.
   */
  STAGE_ATTEMPT_STATE_SCHEDULED = 30,
  /**
   * STAGE_ATTEMPT_STATE_RUNNING - This state indicates that the StageAttempt is now actually being executed
   * by an Executor.
   *
   * This state can transition to COMPLETE or INCOMPLETE.
   */
  STAGE_ATTEMPT_STATE_RUNNING = 40,
  /**
   * STAGE_ATTEMPT_STATE_CANCELLING - This state indicates that the StageAttempt has been cancelled, but this has
   * not yet been communicated to the running STAGE_ATTEMPT.
   *
   * The StageAttempt must be assumed to internally consider itself to be
   * RUNNING while viewing this state from the API - this means that the
   * `running` heartbeat and timeout still apply during this state.
   *
   * This state will transition to TEARING_DOWN as soon as TurboCI has
   * confirmation that the StageAttempt knows it's been cancelled.
   *
   * This state is explicitly different than TEARING_DOWN to avoid the
   * possibility that a Stage is cancelled, but the Stage Attempt doesn't see
   * this until its next valid heartbeat - thus effectively losing up to 1.9x
   * the heartbeat interval out of its TEARING_DOWN timeout.
   */
  STAGE_ATTEMPT_STATE_CANCELLING = 50,
  /**
   * STAGE_ATTEMPT_STATE_TEARING_DOWN - This state indicates that the StageAttempt is doing some work after the
   * 'RUNNING' state.
   *
   * This is meant to model things like:
   *   * doing best-effort cleanup
   *   * exporting logs or other state to other systems
   *   * things like swarming's isolated upload/cache cleanup phase
   *
   * This state will only be used if the stage attempt has execution policy with
   * a non-zero timeout for this state.
   *
   * This state can transition to COMPLETE or INCOMPLETE.
   */
  STAGE_ATTEMPT_STATE_TEARING_DOWN = 60,
  /**
   * STAGE_ATTEMPT_STATE_COMPLETE - This is a final state which indicates that this StageAttempt finished
   * everything the Executor intended it to do.
   *
   * NOTE: 'finished everything the Executor intended it to do' does not imply
   * workflow-level success criteria such as "all tests passed" or "the build
   * successfully compiled". This is more like "the test harness ran all of the
   * tests (regardless of their outcome)" or "the build started and all possible
   * compilation actions were done". These outcomes are highy varied and nuanced
   * (e.g. tests may be graded on flakiness, not simply pass/fail, build may
   * have failed on an optional target but successfully completed the required
   * targets). These workflow-level outcomes are reflected in the Check Results
   * which this StageAttempt would have written during its execution.
   *
   * NOTE: It is currently allowed for an Executor to request a retry of this
   * Stage, even as it marks the StageAttempt as COMPLETE. This can happen in
   * cases where the workflow-level criteria failed and the Executor wants a
   * 'quick and dirty' retry. More nuanced retries should be done at the
   * workflow level by adding additional follow-up Stages which can e.g. just
   * retry the subset of failing tests/build targets, or wait until a wider
   * analysis of other, parallel, stages have completed to make a smarter retry
   * decision.
   *
   * This state is terminal (but if a retry was requested, a new StageAttempt
   * may be created for this Stage).
   */
  STAGE_ATTEMPT_STATE_COMPLETE = 70,
  /**
   * STAGE_ATTEMPT_STATE_INCOMPLETE - This is a final state which indicates that this StageAttempt was not able
   * to complete everything that the Executor intended it to do.
   *
   * The Orchestrator itself can transition the StageAttempt to this state in
   * the event of a heartbeat timeout. This state can also be entered by
   * explicit external actions (e.g. the Executor marking the stage as
   * INCOMPLETE, or external cancellation signals).
   *
   * Like COMPLETE, a retry can be requested for this state, even when the state
   * is explicitly written by the Executor. Similarly, the Executor could mark
   * the StageAttempt as INCOMPLETE without a retry for e.g. a non-retriable
   * failure (e.g. lack of resources).
   *
   * This state is terminal (but if a retry was requested, a new StageAttempt
   * may be created for this Stage).
   */
  STAGE_ATTEMPT_STATE_INCOMPLETE = 80,
  /**
   * STAGE_ATTEMPT_STATE_AWAITING_RETRY - This is an initial state and indicates that a previous StageAttempt went to
   * a terminal state (COMPLETE or INCOMPLETE), a retry was requested, AND the
   * Stage's execution policy permitted a retry.
   *
   * Transition to PENDING is automatic after the execution policy's computed
   * retry delay.
   *
   * This state can transition to PENDING or INCOMPLETE.
   */
  STAGE_ATTEMPT_STATE_AWAITING_RETRY = 90,
}

export function stageAttemptStateFromJSON(object: any): StageAttemptState {
  switch (object) {
    case 0:
    case "STAGE_ATTEMPT_STATE_UNKNOWN":
      return StageAttemptState.STAGE_ATTEMPT_STATE_UNKNOWN;
    case 10:
    case "STAGE_ATTEMPT_STATE_PENDING":
      return StageAttemptState.STAGE_ATTEMPT_STATE_PENDING;
    case 30:
    case "STAGE_ATTEMPT_STATE_SCHEDULED":
      return StageAttemptState.STAGE_ATTEMPT_STATE_SCHEDULED;
    case 40:
    case "STAGE_ATTEMPT_STATE_RUNNING":
      return StageAttemptState.STAGE_ATTEMPT_STATE_RUNNING;
    case 50:
    case "STAGE_ATTEMPT_STATE_CANCELLING":
      return StageAttemptState.STAGE_ATTEMPT_STATE_CANCELLING;
    case 60:
    case "STAGE_ATTEMPT_STATE_TEARING_DOWN":
      return StageAttemptState.STAGE_ATTEMPT_STATE_TEARING_DOWN;
    case 70:
    case "STAGE_ATTEMPT_STATE_COMPLETE":
      return StageAttemptState.STAGE_ATTEMPT_STATE_COMPLETE;
    case 80:
    case "STAGE_ATTEMPT_STATE_INCOMPLETE":
      return StageAttemptState.STAGE_ATTEMPT_STATE_INCOMPLETE;
    case 90:
    case "STAGE_ATTEMPT_STATE_AWAITING_RETRY":
      return StageAttemptState.STAGE_ATTEMPT_STATE_AWAITING_RETRY;
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum StageAttemptState");
  }
}

export function stageAttemptStateToJSON(object: StageAttemptState): string {
  switch (object) {
    case StageAttemptState.STAGE_ATTEMPT_STATE_UNKNOWN:
      return "STAGE_ATTEMPT_STATE_UNKNOWN";
    case StageAttemptState.STAGE_ATTEMPT_STATE_PENDING:
      return "STAGE_ATTEMPT_STATE_PENDING";
    case StageAttemptState.STAGE_ATTEMPT_STATE_SCHEDULED:
      return "STAGE_ATTEMPT_STATE_SCHEDULED";
    case StageAttemptState.STAGE_ATTEMPT_STATE_RUNNING:
      return "STAGE_ATTEMPT_STATE_RUNNING";
    case StageAttemptState.STAGE_ATTEMPT_STATE_CANCELLING:
      return "STAGE_ATTEMPT_STATE_CANCELLING";
    case StageAttemptState.STAGE_ATTEMPT_STATE_TEARING_DOWN:
      return "STAGE_ATTEMPT_STATE_TEARING_DOWN";
    case StageAttemptState.STAGE_ATTEMPT_STATE_COMPLETE:
      return "STAGE_ATTEMPT_STATE_COMPLETE";
    case StageAttemptState.STAGE_ATTEMPT_STATE_INCOMPLETE:
      return "STAGE_ATTEMPT_STATE_INCOMPLETE";
    case StageAttemptState.STAGE_ATTEMPT_STATE_AWAITING_RETRY:
      return "STAGE_ATTEMPT_STATE_AWAITING_RETRY";
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum StageAttemptState");
  }
}
